{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. General info \n",
    "- What you need to run\n",
    "  1. Cell 3.1.   --> pip installs\n",
    "  2. Cell 4.3.   --> pdf generation from text\n",
    "  3. Cell 4.5.   --> Loading the model using transformers (like it was in the demo given to Caspar initially)\n",
    "  4. \n",
    "\n",
    "     \n",
    "- When loading the model, generates the following error, try to restart the kernel and retry:\n",
    "\n",
    "  Error: _Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n",
    "          the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n",
    "          these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n",
    "          `device_map` to `from_pretrained`. Check\n",
    "          https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n",
    "          for more details._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Useful documentation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "- Chatbot Arena Leaderboard from HuggingFace can be found [here](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)\n",
    "- __LangChain__: [Stuff, Map-Reduce & Refine](https://python.langchain.com/docs/use_cases/summarization)\n",
    "- __LangChain__: [Quick start](https://python.langchain.com/docs/get_started/quickstart)\n",
    "- __LangChain__ __HuggingFace__: [Click here](https://python.langchain.com/docs/integrations/chat/huggingface)\n",
    "  1) Utilize the HuggingFaceTextGenInference, HuggingFaceEndpoint, or HuggingFaceHub integrations to instantiate an LLM.\n",
    "  2) Utilize the ChatHuggingFace class to enable any of these LLMs to interface with LangChain’s Chat Messages abstraction.\n",
    "  3) Demonstrate how to use an open-source LLM to power an ChatAgent pipeline\n",
    "- __AutoModelForCausalLM__: the way we managed to load the dutch model for LangChain processing is described [here](https://python.langchain.com/docs/integrations/llms/huggingface_pipelines)\n",
    "- __LlamaCpp__: [Parameters description](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.llamacpp.LlamaCpp.html#)\n",
    "- __Google Cloud Generative AI__ - Language __GitHub__ repository: [Click here](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.ipynb)  \n",
    "- __Medium__ article: [Generating Summaries for Large Documents with Llama2 using Hugging Face and Langchain](https://medium.com/@ankit941208/generating-summaries-for-large-documents-with-llama2-using-hugging-face-and-langchain-f7de567339d2)\n",
    "- __Medium__ article: [Long Text Summarization, RetrievalQA and Vector Databases -LangChain Arxiv Tutor](https://medium.com/@baptisteloquette.entr/langchain-arxiv-tutor-long-text-summarization-retrievalqa-and-vector-databases-6d5cb1dc7e14)\n",
    "- __Medium__ article: [Retreaval Augmented Generation) architecture](https://pub.towardsai.net/advanced-rag-02-unveiling-pdf-parsing-b84ae866344e)\n",
    "- __Medium__ article: [Prompt Engineering: How to Trick AI into Solving Your Problems](https://towardsdatascience.com/prompt-engineering-how-to-trick-ai-into-solving-your-problems-7ce1ed3b553f)\n",
    "- __Medium__ article: [Llama 2 Prompt Engineering: Extracting Information From Articles Examples](https://medium.com/@eboraks/llama-2-prompt-engineering-extracting-information-from-articles-examples-45158ff9bd23)\n",
    "- [Pinecone LangChain AI Handbook](https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/#ConversationChain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Pip installs, Different Inputs (Pdf generation, Input texts, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3.1. Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZFUPwRcLRBTO",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.1/330.1 kB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed fsspec-2024.2.0 huggingface-hub-0.20.3 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 tqdm-4.66.2 transformers-4.38.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.27.2-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-0.27.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting scipy (from bitsandbytes)\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.29.0,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->bitsandbytes) (1.24.1)\n",
      "Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy, bitsandbytes\n",
      "Successfully installed bitsandbytes-0.42.0 scipy-1.12.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.9-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
      "  Downloading SQLAlchemy-2.0.27-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
      "  Downloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.21 (from langchain)\n",
      "  Downloading langchain_community-0.0.22-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting langchain-core<0.2,>=0.1.26 (from langchain)\n",
      "  Downloading langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain)\n",
      "  Downloading langsmith-0.1.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.24.1)\n",
      "Collecting pydantic<3,>=1 (from langchain)\n",
      "  Downloading pydantic-2.6.1-py3-none-any.whl.metadata (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain)\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.26->langchain) (4.0.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.26->langchain) (23.2)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.2 (from pydantic<3,>=1->langchain)\n",
      "  Downloading pydantic_core-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic<3,>=1->langchain)\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain)\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.1.3)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain-0.1.9-py3-none-any.whl (816 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m141.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_community-0.0.22-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.1.26-py3-none-any.whl (246 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.1.5-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.6.1-py3-none-any.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.8/394.8 kB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading SQLAlchemy-2.0.27-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: typing-extensions, tenacity, mypy-extensions, multidict, marshmallow, jsonpatch, greenlet, frozenlist, async-timeout, annotated-types, yarl, typing-inspect, SQLAlchemy, pydantic-core, aiosignal, pydantic, dataclasses-json, aiohttp, langsmith, langchain-core, langchain-community, langchain\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed SQLAlchemy-2.0.27 aiohttp-3.9.3 aiosignal-1.3.1 annotated-types-0.6.0 async-timeout-4.0.3 dataclasses-json-0.6.4 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 langchain-0.1.9 langchain-community-0.0.22 langchain-core-0.1.26 langsmith-0.1.5 marshmallow-3.20.2 multidict-6.0.5 mypy-extensions-1.0.0 pydantic-2.6.1 pydantic-core-2.16.2 tenacity-8.2.3 typing-extensions-4.9.0 typing-inspect-0.9.0 yarl-1.9.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/lib/python3/dist-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=65e69dcede2e8ccba6d73bb758c41d35635d10e46cdcc08620d92208377d55a5\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.6.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-4.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Downloading pypdf-4.0.2-py3-none-any.whl (283 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-4.0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting reportlab\n",
      "  Downloading reportlab-4.1.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from reportlab) (9.3.0)\n",
      "Collecting chardet (from reportlab)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Downloading reportlab-4.1.0-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: chardet, reportlab\n",
      "Successfully installed chardet-5.2.0 reportlab-4.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m160.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m146.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.0 pytz-2024.1 tzdata-2024.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting streamlit\n",
      "  Downloading streamlit-1.31.1-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: importlib-metadata<8,>=1.4 in /usr/lib/python3/dist-packages (from streamlit) (4.6.4)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.24.1)\n",
      "Requirement already satisfied: packaging<24,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (23.2)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.0)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.3.0)\n",
      "Collecting protobuf<5,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit)\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.9.0)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit)\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit)\n",
      "  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
      "Collecting watchdog>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
      "Collecting toolz (from altair<6,>=4.0->streamlit)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2022.12.7)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14,>=10.14.0->streamlit)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading streamlit-1.31.1-py2.py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m154.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: watchdog, validators, tzlocal, toolz, toml, smmap, pyarrow, protobuf, mdurl, click, cachetools, pydeck, markdown-it-py, gitdb, rich, gitpython, altair, streamlit\n",
      "Successfully installed altair-5.2.0 cachetools-5.3.2 click-8.1.7 gitdb-4.0.11 gitpython-3.1.42 markdown-it-py-3.0.0 mdurl-0.1.2 protobuf-4.25.3 pyarrow-15.0.0 pydeck-0.8.1b0 rich-13.7.0 smmap-5.0.1 streamlit-1.31.1 toml-0.10.2 toolz-0.12.1 tzlocal-5.2 validators-0.22.0 watchdog-4.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Installing time: 56.3 sec\n"
     ]
    }
   ],
   "source": [
    "# download the gguf file for the Llama Dutch model, when Cpp method is used \n",
    "#!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n",
    "#!huggingface-cli download TheBloke/Llama-2-13B-GGUF llama-2-13b.Q6_K.gguf --local-dir . --local-dir-use-symlinks False\n",
    "import time\n",
    "start_time = time.time()\n",
    "!pip install transformers\n",
    "!pip install --upgrade transformers\n",
    "!pip install accelerate                 # Necessary for llama model\n",
    "!pip install bitsandbytes\n",
    "!pip install langchain\n",
    "!pip install huggingface_hub\n",
    "!pip install langdetect\n",
    "!pip install tiktoken\n",
    "!pip install pypdf\n",
    "!pip install reportlab\n",
    "!pip install pandas\n",
    "!pip install streamlit\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Installing time: {end_time-start_time:.1f} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This was an attempt in order to be able to load the model for LangChain (in the end this is not been used)\n",
    "!huggingface-cli download BramVanroy/Llama-2-13b-chat-dutch --local-dir ./Bram --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!huggingface-cli download meta-llama/Llama-2-13b-chat-hf --local-dir ./Bram --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check if this cell is still needed or not:\n",
    "!pip install google-cloud-translate\n",
    "\n",
    "# T5 needs the following package:\n",
    "!pip install sentencepiece\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the requirements.txt based on the installed packages\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Different Inputs: \n",
    "3.2.1. text  \n",
    "3.2.2. pdf  \n",
    "3.2.3. text transformer to pdf  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.2.1. Text (6p): [Landelijk dekkend netwerk van infrastructuren](https://test.polpo.nl/nl/document/eb024e5c-7779-4d2c-9b63-b6e2fe0a2431)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = '''Landelijk dekkend netwerk van infrastructuren\n",
    "Origineel: Tweede KamerDatum: 21-01-2024\n",
    "Tweede Kamer der Staten-Generaal\n",
    "2\n",
    "Vergaderjaar 2023–2024\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "27 529 Informatie- en Communicatietechnologie (ICT) in\n",
    "de Zorg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Nr. 313 BRIEF VAN DE MINISTER VAN VOLKSGEZONDHEID, WELZIJN EN\n",
    "SPORT\n",
    "\n",
    "Aan de Voorzitter van de Tweede Kamer der Staten-Generaal\n",
    "\n",
    "Den Haag, 22 januari 2024\n",
    "\n",
    "De gezondheidszorg staat onder grote druk; door de vergrijzing groeit de\n",
    "vraag naar zorg, terwijl er minder capaciteit beschikbaar is door een\n",
    "toenemend personeelstekort. De regeldruk en administratieve lasten zijn\n",
    "hoog en zorgverleners zijn veel tijd kwijt aan het verzamelen, overnemen\n",
    "en invoeren van de benodigde informatie voor het verlenen van goede\n",
    "zorg. De zorgverlening vindt in toenemende mate plaats in een netwerk\n",
    "van meerdere partijen, wat beter ondersteund moet worden met snelle en\n",
    "veilige gegevensuitwisseling. Tegelijkertijd groeit de behoefte aan data\n",
    "voor secundair gebruik ten behoeve van wetenschappelijk onderzoek,\n",
    "kwaliteitsmonitoring, gezondheidsbeleid en AI-toepassingen. De noodzaak\n",
    "om een landelijk dekkend netwerk van infrastructuren – voor de volle\n",
    "breedte van de zorg – te realiseren, voor het uitwisselen en beschikbaar\n",
    "stellen van relevante data aan patiënt, zorgverlener en onderzoeker,\n",
    "neemt toe. Daarom heeft het zorgveld aan het Ministerie van VWS\n",
    "gevraagd regie te pakken en sturing te geven aan de realisatie van een\n",
    "landelijk dekkend netwerk.\n",
    "\n",
    "In de brief van mijn ambtsvoorganger van 13 april 2023 over het Landelijk\n",
    "dekkend netwerk van infrastructuren1 heeft hij uw Kamer geïnformeerd\n",
    "over de analyse, die op zijn verzoek is uitgevoerd, naar scenario's die\n",
    "aanvullend op het huidige beleid een toekomstbestendig landelijk\n",
    "dekkend netwerk realiseren. Hieruit kwam het advies om gelijktijdig een\n",
    "dubbele beweging te omarmen en te stimuleren:\n",
    "Een gedistribueerd communicatienetwerk voor (geprotocolleerde)\n",
    "overdracht van gegevens door één-op-één communicatie tussen\n",
    "zorgverleners én om data beschikbaar te kunnen stellen voor andere\n",
    "doeleinden.\n",
    "\n",
    "\n",
    "\n",
    "1\n",
    "Kamerstukken II 2022/23, 27 529, nr. 293\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kst-27529-313\n",
    "ISSN 0921 - 7371\n",
    "'s-Gravenhage 2024 Tweede Kamer, vergaderjaar 2023–2024, 27 529, nr. 313 1\n",
    "Een data-centrische oplossing, bestaande uit gekoppelde dataplatfor-\n",
    "men, die het gebruik van data scheidt van de functionaliteit en\n",
    "databeschikbaarheid voor primair en secundair gebruik, alsmede\n",
    "gezamenlijke dossiervorming in de context van netwerkzorg, faciliteert.\n",
    "Mijn ambtsvoorganger heeft daarbij aangegeven beide geadviseerde\n",
    "richtingen, met de kennis en expertise van veldpartijen, verder uit te laten\n",
    "werken. In deze brief geef ik invulling aan de toezegging om uw Kamer te\n",
    "informeren over de uitwerking van de geadviseerde richting en de te\n",
    "nemen vervolgstappen om tot een landelijk dekkend netwerk te komen.\n",
    "\n",
    "De infrastructuur moet de uitdagingen van de zorg ondersteunen\n",
    "\n",
    "De technologische ontwikkelingen gaan razendsnel en bieden veel\n",
    "potentie voor het verbeteren van de kwaliteit van de zorg en het vermin-\n",
    "deren van de administratieve lasten. Er worden regionaal allerlei\n",
    "initiatieven gestart om toepassingen te ontwikkelen, die netwerk- en\n",
    "hybride zorg kunnen ondersteunen. Belangrijke innovaties, want met de\n",
    "juiste en volledige data kan een zorgverlener betere en veilige zorg\n",
    "leveren. En kunnen burgers meebeslissen over voor hen passende zorg.\n",
    "We moeten echter voorkomen dat de huidige versnippering in het\n",
    "zorglandschap continueert en zelfs versterkt wordt, door allerlei nieuwe\n",
    "initiatieven met eigen standaarden die niet onderling interoperabel zijn. Er\n",
    "is behoefte aan een solide en toekomstbestendige landelijke infra-\n",
    "structuur, waarmee het zorgproces ondersteund wordt en waarop verdere\n",
    "technologische ontwikkelingen kunnen plaats vinden. Open internationale\n",
    "standaarden moeten hierbij het uitgangspunt vormen, opdat er geprofi-\n",
    "teerd kan worden van internationale kennis en innovaties en de data\n",
    "uiteindelijk niet alleen landelijk maar ook Europees via de European\n",
    "Health Data Space (EHDS) kan stromen.\n",
    "\n",
    "In de vorige brief is benoemd dat zorginformatie vaak binnen één bepaald\n",
    "zorgproces wordt vastgelegd en gedeeld; er vindt een geprotocolleerde\n",
    "overdracht van gegevens plaats. Denk hierbij aan een verwijzing van een\n",
    "huisarts naar een specialist, of de overdracht van ziekenhuis naar\n",
    "verpleeghuis of wijkverpleging. De geprioriteerde gegevensuitwisselingen\n",
    "onder de Wet elektronische gegevensuitwisseling in de zorg (Wegiz)\n",
    "ondersteunen dit. In de Nationale visie en strategie op het gezondheidsin-\n",
    "formatiestelsel2 (NVS) wordt het belang van deze geprotocolleerde\n",
    "overdrachten onderschreven, maar wordt ook onderkend dat alleen het\n",
    "realiseren van een infrastructuur voor geprotocolleerde gegevensuit-\n",
    "wisseling niet voldoende is om zorgverleners te faciliteren bij het verlenen\n",
    "van hybride en netwerkzorg. Daarom willen we van gegevensuitwisseling\n",
    "doorgroeien naar databeschikbaarheid.\n",
    "\n",
    "Een communicatienetwerk voor gegevensuitwisseling\n",
    "\n",
    "Voor gegevensuitwisseling is een communicatienetwerk nodig. In het\n",
    "geadviseerde scenario (een gedistribueerd communicatienetwerk) heeft\n",
    "elke zorgaanbieder zijn eigen knooppunt en kunnen zorgaanbieders\n",
    "onderling tussen deze knooppunten gegevens uitwisselen zonder\n",
    "tussenkomst van een derde partij.\n",
    "\n",
    "De verdere uitwerking van dit scenario en de afstemming hierover met\n",
    "veldpartijen heeft tot de conclusie geleid dat dit scenario momenteel een\n",
    "nog te grote stap is. Aangezien veel zorgaanbieders al zijn aangesloten op\n",
    "een zorginfrastructuur (bijvoorbeeld LSP en Chipsoft Zorgplatform) is het\n",
    "efficiënter om eerst toe te werken naar een hybride situatie. In deze\n",
    "hybride situatie ontsluiten bestaande infrastructuren de data bij hun\n",
    "\n",
    "2\n",
    "Kamerstukken II, 2022/23, 27 529, nr. 292\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tweede Kamer, vergaderjaar 2023–2024, 27 529, nr. 313 2\n",
    "gekoppelde zorgaanbieders en stellen die gegevens vervolgens\n",
    "beschikbaar via een (gezamenlijk) knooppunt. Een zorgaanbieder zonder\n",
    "zorginfrastructuur kan een eigen knooppunt gebruiken. De verzameling\n",
    "van al deze knooppunten vormt het communicatienetwerk.\n",
    "Dit biedt alle zorgaanbieders de mogelijkheid om onderling (rechtstreeks\n",
    "via een eigen knooppunt of via een bestaande infrastructuur) gegevens uit\n",
    "te wisselen.\n",
    "Om deze knooppunten te ontwikkelen en verbinden is het noodzakelijk om\n",
    "te komen tot een landelijke kaderstelling voor standaardisatie van taal en\n",
    "techniek, in de vorm van een landelijk vertrouwensstelsel (LVS)3. Het LVS\n",
    "omvat het geheel aan technische, organisatorische en juridische\n",
    "afspraken die zorgt voor vertrouwen in de landelijke elektronische\n",
    "gegevensuitwisseling en het gebruik van gezondheidsgegevens.\n",
    "Generieke functies en bijbehorende voorzieningen vormen een belangrijk\n",
    "onderdeel van dit stelsel. In de recente Kamerbrief over Generieke\n",
    "functies4 benoemt mijn ambtsvoorganger de interventies die nodig zijn\n",
    "om de randvoorwaardelijke generieke functies en voorzieningen te\n",
    "realiseren.\n",
    "\n",
    "Doorgroeien naar databeschikbaarheid\n",
    "\n",
    "We willen dat patiënten, zorgverleners en onderzoekers digitaal kunnen\n",
    "beschikken over de juiste informatie, op het juiste moment en op de juiste\n",
    "plek. Hiervoor zijn toepassingen nodig die data beschikbaar stellen voor\n",
    "preventie, het primaire zorgproces en secundair datagebruik5. Deze\n",
    "toepassingen maken gebruik van het communicatienetwerk voor\n",
    "gegevensuitwisseling om bij meerdere betrokken zorgaanbieders\n",
    "(tegelijkertijd) actuele en relevante zorggegevens op te vragen.\n",
    "\n",
    "Veel initiatieven voor dergelijke toepassingen stagneren omdat het lastig\n",
    "is om op een eenduidige wijze bij verschillende zorgaanbieders data te\n",
    "ontsluiten en samen te voegen. Dit komt doordat de verschillende\n",
    "infrastructuren en knooppunten nog niet met elkaar verbonden zijn, maar\n",
    "ook doordat zorgaanbieders hun data nog niet (voldoende) gestandaardi-\n",
    "seerd vastleggen.\n",
    "\n",
    "Er wordt vaak gesproken over data uit de bron, maar «de bron» is niet\n",
    "altijd geschikt voor (gestandaardiseerde) dataopslag en -ontsluiting. Het\n",
    "Elektronisch Patiënten Dossier (EPD) en Elektronisch Cliënt Dossier (ECD)\n",
    "zijn systemen voor zorgaanbieders, die initieel zijn toegespitst op het\n",
    "functioneel gebruik binnen de organisatie. Om tijdig de juiste data voor\n",
    "gegevensdeling beschikbaar te stellen, moet een EPD/ECD systeem\n",
    "functionaliteit en data goed van elkaar kunnen scheiden. Tevens is er niet\n",
    "altijd sprake van één bronsysteem; in sommige sectoren werken\n",
    "zorgaanbieders met meerdere applicaties. Met name in de tweede-\n",
    "lijnszorg kiezen steeds meer zorgaanbieders ervoor om data uit één of\n",
    "meerdere bronsystemen separaat op te slaan (datawarehouse of data\n",
    "lake). De data wordt in principe onbewerkt en ongestructureerd\n",
    "gekopieerd vanuit de bron. De vertaling van die data naar een gestandaar-\n",
    "diseerd datamodel kan in dezelfde opslagomgeving plaatsvinden – men\n",
    "spreekt dan over een dataplatform – of wordt als separate functionaliteit\n",
    "los van de opslag uitgevoerd. Wanneer een EPD- of ECD-systeem niet de\n",
    "data volgens het afgesproken informatiemodel op kan slaan en/of als er\n",
    "sprake is van meerdere bronsystemen, kan een dataplatform een\n",
    "\n",
    "3\n",
    "Eén van de randvoorwaarden voor gegevensuitwisseling zoals beschreven in de brief van\n",
    "13 april 2023\n",
    "4\n",
    "Kamerstukken II, 2022/23, 27 529, nr. 312\n",
    "5\n",
    "Mits daarvoor toestemming is gegeven door de patiënt en aan alle voorwaarden ten aanzien\n",
    "van identificatie, authenticatie en autorisatie is voldaan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tweede Kamer, vergaderjaar 2023–2024, 27 529, nr. 313 3\n",
    "gewenste aanvulling zijn voor een zorgaanbieder. Daarbij is het\n",
    "uitgangspunt dat de data onder de verantwoordelijkheid en invloedssfeer\n",
    "van de zorgaanbieder blijft.\n",
    "\n",
    "Inspringen op een behoefte: netwerk- en integratiediensten\n",
    "\n",
    "Na het verder uitwerken van de twee geadviseerde scenario's, vanuit het\n",
    "architectuur- en techniekperspectief, zijn de contouren getoetst in de\n",
    "praktijk. Aanbieders van regionale en sectorale toepassingen, die een\n",
    "bijdrage leveren aan het realiseren van databeschikbaarheid, hebben de\n",
    "functionaliteit van hun oplossing schriftelijk beschreven en vervolgens\n",
    "persoonlijk toegelicht. Deze gesprekken en documentatie hebben\n",
    "inzichtelijk gemaakt waar de uitdagingen en knelpunten zitten.\n",
    "\n",
    "De standaardisatie van taal en techniek gaat niet snel genoeg om de\n",
    "toenemende vraag naar toepassingen voor databeschikbaarheid te\n",
    "faciliteren. Aanbieders van toepassingen moeten zorgaanbieders (en\n",
    "diens leveranciers) ondersteunen bij het ontsluiten van de data en\n",
    "daarnaast nog allerlei netwerk- en integratiediensten (bijvoorbeeld het\n",
    "verzamelen, vertalen, valideren, samenvoegen en verrijken van data)\n",
    "aanbieden om de data bruikbaar te maken voor de toepassingen. Nu\n",
    "worden al deze «diensten» voor elke toepassing separaat en opnieuw\n",
    "ontwikkeld en uitgevoerd; hiermee gaat veel tijd en geld verloren. Ook\n",
    "moet voorkomen worden dat databeschikbaarheid een verdienmodel\n",
    "wordt; het hele proces van data lokaliseren, opvragen, beschikbaar\n",
    "stellen, verzamelen en integreren moet non-concurrentieel zijn. Toepas-\n",
    "singen en diensten daarentegen mogen – met name voor de meer\n",
    "specifieke functionaliteit – wel concurrentieel zijn om ontwikkeling en\n",
    "innovatie te bevorderen.\n",
    "\n",
    "Inmiddels hebben een aantal partijen en zorgkoepels6 hun ambities en\n",
    "krachten gebundeld in de CumuluZ-coalitie met als doel toe te werken\n",
    "naar een landelijke data infrastructuur met één non-concurrentieel\n",
    "data-integratieplatform, die de data bij zorgaanbieders ontsluit, verwerkt\n",
    "en vervolgens beschikbaar stelt aan toepassingen en diensten.\n",
    "\n",
    "Aanscherping van de voorgestelde scenario's\n",
    "\n",
    "Zoals in het begin van deze brief benoemd zijn de volgende twee\n",
    "scenario's nader uitgewerkt:\n",
    "Een gedistribueerd communicatienetwerk voor (geprotocolleerde)\n",
    "overdracht van gegevens door één-op-één communicatie tussen\n",
    "zorgverleners én om data beschikbaar te kunnen stellen voor andere\n",
    "doeleinden.\n",
    "Een data-centrische oplossing, bestaande uit gekoppelde dataplatfor-\n",
    "men, die het gebruik van data scheidt van de functionaliteit en\n",
    "databeschikbaarheid voor primair en secundair gebruik, alsmede\n",
    "gezamenlijke dossiervorming in de context van netwerkzorg, faciliteert.\n",
    "\n",
    "De uitwerking heeft inzichtelijk gemaakt waar de knelpunten zitten en\n",
    "welke interventies nodig zijn om tot landelijke gegevensuitwisseling en\n",
    "databeschikbaarheid te komen. Vervolgens zijn de zorgkoepels en\n",
    "adviesorganen geconsulteerd over voorgestelde aanscherpingen op de\n",
    "geadviseerde scenario's.\n",
    "Deze aanscherpingen zijn:\n",
    "\n",
    "\n",
    "\n",
    "6\n",
    "Nederlandse Federatie van Universitair Medische Centra (NFU), Nederlandse Vereniging van\n",
    "Ziekenhuizen (NVZ), Santeon en mProve\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tweede Kamer, vergaderjaar 2023–2024, 27 529, nr. 313 4\n",
    "Communicatienetwerk\n",
    "Om binnen de IZA-termijn te komen tot landelijke gegevensuit-\n",
    "wisseling wordt gekozen voor een hybride oplossing waarbij\n",
    "bestaande infrastructuren worden verbonden met elkaar en met\n",
    "zorgaanbieders die geen onderdeel uitmaken van een specifieke\n",
    "infrastructuur, maar een eigen knooppunt hebben.\n",
    "o Verbinden van bestaande infrastructuren: mijn ministerie zorgt voor\n",
    "het opstellen van «technical agreements» (TA's) per uitwisselings-\n",
    "vorm, waarmee bestaande infrastructuren met knooppunten\n",
    "verbonden worden. Hiermee zullen (leveranciers van) zorgaanbie-\n",
    "ders van bijvoorbeeld ziekenhuiszorg, wijkverpleging en verpleeg-\n",
    "zorg, de verbinding van infrastructuren beproeven en vervolgens in\n",
    "gebruik nemen. De overdracht vindt plaats op basis van FHIR en\n",
    "maakt gebruik van huidige landelijke afsprakenstelsels (zoals\n",
    "MedMij, Twiin en Health-RI) en het TwiinxNuts groeipad dat richting\n",
    "2025 wordt ontwikkeld onder het Landelijk vertrouwensstelsel;\n",
    "o Infrastructuur voor uitwisseling van medische beelden: de DVD-exit\n",
    "infrastructuur (het Twiin portaal) wordt ingezet als tijdelijke\n",
    "oplossing, inclusief lokalisatiefunctie voor de benodigde historische\n",
    "zorgtijdslijn.\n",
    "\n",
    "Data- en integratieplatform(en)\n",
    "Het uitgangspunt voor dataopslag is dat data wordt opgeslagen onder\n",
    "verantwoordelijkheid en invloedssfeer van de zorgaanbieder.\n",
    "o Hoe de data opgeslagen wordt (bronsysteem of datawarehouse/\n",
    "data lake/dataplatform) en waar de vertaling naar landelijke\n",
    "informatiestandaarden plaats vindt, is afhankelijk van de mogelijk-\n",
    "heden en keuzes van de zorgaanbieder (en haar leverancier);\n",
    "o Alleen centrale opslag van data (data van meerdere zorgaanbieders\n",
    "opgeslagen op één en dezelfde locatie) wanneer de noodzaak\n",
    "aangetoond is en er een juridische grondslag vanuit de Algemene\n",
    "Verordening Gegevensbescherming (AVG) en de Wet op de\n",
    "geneeskundige behandelovereenkomst (Wgbo) voor aanwezig is.\n",
    "Inzetten op integratie- en netwerkdiensten om versnelling te realiseren.\n",
    "o Kom tot een – op open internationale standaarden gebaseerde –\n",
    "landelijke data infrastructuur voor primair en secundair gebruik,\n",
    "waarbij het CumuluZ-concept als uitgangspunt dient voor een\n",
    "non-concurrentiële data-integratie laag;\n",
    "o Groei toe naar een publieke voorziening voor data-integratie laag\n",
    "met daarbij de benodigde integratie en netwerkdiensten, die\n",
    "bijdraagt aan de realisatie van databeschikbaarheid voor het hele\n",
    "gezondheidsinformatiestelsel;\n",
    "o Maak (her)gebruik van de kennis, ervaring en functionaliteit die\n",
    "aanwezig is bij de bewezen initiatieven en werk actief samen om\n",
    "die initiatieven te harmoniseren en uiteindelijk te integreren in één\n",
    "infrastructuur;\n",
    "o Start met regionale en sectorale initiatieven die aantoonbaar\n",
    "schaalbaar zijn en ga die na een succesvolle beproeving landelijk\n",
    "implementeren;\n",
    "o Investeer alleen in nieuwe initiatieven als die nieuwe functionaliteit\n",
    "voor de zorgsector toevoegen en passen in de beoogde\n",
    "doelarchitectuur;\n",
    "o Integratie- en netwerkdiensten zijn ter aanvulling en niet ter\n",
    "vervanging van de benodigde eenheid van taal en techniek. Vanuit\n",
    "kwaliteit, verantwoordelijkheid en kosten perspectief blijft gestan-\n",
    "daardiseerde vastlegging de verantwoordelijkheid van de zorgaan-\n",
    "bieder (die hierin gefaciliteerd moet worden door de leverancier\n",
    "van het bronsysteem) en dient dit zo dicht mogelijk bij de bron\n",
    "(zorgverlener) plaats te vinden.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tweede Kamer, vergaderjaar 2023–2024, 27 529, nr. 313 5\n",
    "De leden van het Informatieberaad zorg hebben hun steun uitgesproken\n",
    "voor deze koers met de aanscherpingen op de twee scenario's. Dit moet\n",
    "de komende maanden leiden tot een verbreding van de CumuluZ-coalitie,\n",
    "met een vertegenwoordiging uit meerdere zorgdomeinen en het inregelen\n",
    "van de governance van de coalitie (incl. juridische entiteit en rol van het\n",
    "Ministerie van VWS). Daarnaast wil ik een «werkplaats» inrichten om niet\n",
    "alleen op bestuurlijk niveau (CumuluZ-coalitie), maar vooral ook op\n",
    "inhoud partijen actief samen te laten werken om proeftuinen te initiëren\n",
    "en bestaande, bewezen toepassingen te harmoniseren en integreren in de\n",
    "doelarchitectuur.\n",
    "\n",
    "Vervolg\n",
    "\n",
    "Ik zet de huidige koers van standaardisatie van taal en techniek voort, die\n",
    "nodig is voor in eerste instantie gegevensuitwisseling groeiend naar\n",
    "databeschikbaarheid. Zoals ook benoemd in de kamerbrief over Generieke\n",
    "functies is het noodzakelijk om te komen tot een landelijke kaderstelling,\n",
    "in de vorm van een landelijk vertrouwensstelsel.\n",
    "\n",
    "In aanvulling hierop ga ik samen met de CumuluZ-coalitie de regie voeren\n",
    "op het realiseren van een landelijke data infrastructuur voor primair en\n",
    "secundair gebruik. Daarbij wil ik toegroeien naar een publieke voorziening\n",
    "voor de data-integratie laag, met bijbehorende integratie- en netwerk-\n",
    "diensten, om de doorontwikkeling van gegevensuitwisseling naar\n",
    "databeschikbaarheid te ondersteunen en versnellen. De mogelijkheid,\n",
    "intensiteit en snelheid van deze ambitie moeten worden bezien in\n",
    "samenhang met de middelen die beschikbaar zijn op de aanvullende post\n",
    "bij het Ministerie van Financiën voor het bevorderen van de gegevensuit-\n",
    "wisseling in de zorg. Over de inzet van deze middelen moet nadere\n",
    "besluitvorming plaatsvinden. Ik bericht u na het voorjaar over de stand\n",
    "van zaken.\n",
    "\n",
    "De volgende stap is het uitwerken van de aangescherpte scenario's in een\n",
    "doelarchitectuur en een transitieplan, met concrete acties voor de korte\n",
    "termijn en langere termijn doelen voor de daaropvolgende jaren. Hierbij\n",
    "wordt aangesloten op de besluiten die in het IZA-uitvoeringsakkoord zijn\n",
    "opgenomen en de doelstelling (per plateau) van de NVS. De regie ligt bij\n",
    "mijn ministerie, en de invulling van het plan zal in afstemming met de\n",
    "CumuluZ-coalitie en IZA-partijen plaatsvinden. Voor het einde van dit jaar\n",
    "zal ik uw kamer informeren over de voortgang omtrent de doelstellingen\n",
    "die in 2025 gerealiseerd moeten worden en de doelarchitectuur en het\n",
    "transitieplan.\n",
    "\n",
    "De Minister van Volksgezondheid, Welzijn en Sport,\n",
    "C. Helder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tweede Kamer, vergaderjaar 2023–2024, 27 529, nr. 313 6'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3.2.2. PDF\n",
    "- [Landelijk dekkend netwerk van infrastructuren](https://test.polpo.nl/nl/document/eb024e5c-7779-4d2c-9b63-b6e2fe0a2431)\n",
    "- [Financieringsmonitor2024](https://test.polpo.nl/nl/document/d0f53dc4-a04b-40f4-b48c-b697760a0d7f/startup*%20%7C%20%22start-ups%22%20%7C%20scaleup*%20%7C%20%22scale-ups%22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader(\"LandelijkDekkendNetwerk.pdf\")\n",
    "docs = pdf_loader.load()\n",
    "pages = pdf_loader.load_and_split()\n",
    "print(len(pages))\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 2\n",
    "list_page_content = [page.page_content for page in pages]\n",
    "some_text = ''.join(list_page_content)\n",
    "ntchar = len(some_text)\n",
    "chunk_size = int(ntchar/n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Text to PDF\n",
    "3.2.3.1. Kunstmatige intelligentie  \n",
    "3.2.3.2. History Greek, Dutch and Belgium  \n",
    "\n",
    "- __REQUIREMENT__: You need to import the definition __create_pdf__ from the __Code__ section 4.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3.2.3.1. Elastic search snippets, Kunstmatige Intelligentie [related pdf](https://test.polpo.nl/nl/document/2307c590-3f69-4170-88af-fc0841093623/%22kunstmatige%20intelligentie%22) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet1 = \"Medewerkers gaan we verder ontwikkelen in het toepassen van de menselijke maat in onze dienstverlening. Met ondersteuning van kunstmatige intelligentie gaan we onze brieven leesbaarder en begrijpelijker maken. De benadering is van buiten naar binnen: knelpunten die onze cliënten ervaren worden in kaart gebracht op basis van verschillende vormen van (klant)onderzoek en analyses.\"\n",
    "snippet2 = \"Het programma Innovatie ondersteunt initiatieven en oplosteams in het effectief organiseren van verbetertrajecten en het bedenken van vernieuwende oplossingen, met kennis over de laatste (technologische) ontwikkelingen en trends. Zo wordt onderzocht hoe kunstmatige intelligentie (zoals ChatGPT) ingezet kan worden, bijvoorbeeld bij het herschrijven van algemene teksten in tientallen brieven om de leesbaarheid te verbeteren. Aandacht voor innovatie en design thinking draagt ook bij aan de gewenste ontwikkeling van een lerende organisatie.\"\n",
    "snippet_collection_nl = snippet1 + \"\\n\" + snippet2\n",
    "\n",
    "snippet_collection_en = '''We will further develop employees in applying the human scale in our services. With support of\n",
    "artificial intelligence we will make our letters more readable and make it more understandable. The approach is from the outside in: bottlenecks\n",
    "that our clients experience are mapped on the basis of various forms of (customer) research and analyses. The program\n",
    "Innovation supports initiatives and solution teams effectively organizing improvement processes and devising innovative ones\n",
    "solutions, with knowledge of the latest (technological) developments and trends. For example, it is investigated how artificial intelligence (such as\n",
    "ChatGPT) can be used, for example when rewriting general texts in dozens of letters to improve readability.\n",
    "Attention to innovation and design thinking also contributes to the desired results development of a learning organization.'''\n",
    "create_pdf([snippet_collection_nl], \"KI_nl.pdf\")\n",
    "create_pdf([snippet_collection_en], \"KI_en.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3.2.3.2. History Greek, Dutch and Belgium (text self created using ChatGPT)\n",
    "Depending on the input text of the be, nl, gr fragments the map reduce gives different output. That is why i am creating two versions of the pdfs. The first version is based on the text1 fragments and the second version is based on the text2 fragments. Notice the difference between the text1 and text2 fragments. Text1 uses \\n whereas text2 versions use cariage returns leading to white lines between alineas. Comparing the history1 and history2 pdfs you can not see any difference although the output of the Map-Reduce is highly different. See Map-Reduce output under __Useful Code__ section 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the History texts:\n",
      "(Gr, Nl, Be) = (463, 381, 444)\n"
     ]
    }
   ],
   "source": [
    "be_history_text1 = '''Belgian history is rich and complex, characterized by its strategic location in Western Europe and \\n\n",
    "its cultural diversity. Here's a brief summary:\n",
    "Early History: The region now known as Belgium has been inhabited since prehistoric times. \\n\n",
    "It was later settled by Celtic and Germanic tribes before coming under Roman rule in the first century BC. The area flourished during Roman times as part of the province of Gallia Belgica.\n",
    "Medieval Period: After the fall of the Roman Empire, the region was invaded and settled by various Germanic tribes. In the early Middle Ages, it became part of the Frankish Empire. During this period, the area saw the rise of powerful feudal lords and the emergence of important trading cities like Ghent, Bruges, and Antwerp.\n",
    "Burgundian and Habsburg Rule: In the 15th century, the Burgundian dukes gained control of much of present-day Belgium. This period saw the flourishing of arts and culture, but also increased centralization of power. The region later came under Habsburg rule as part of the Spanish and Austrian Netherlands.\n",
    "Dutch Independence: In the 16th and 17th centuries, the Dutch Revolt against Spanish rule led to the independence of the northern provinces of the Netherlands. However, the southern provinces, including present-day Belgium, remained under Spanish control until they were conquered by France in the late 17th century.\n",
    "French Rule: Belgium became part of France under Napoleon Bonaparte's rule in the early 19th century. During this time, French revolutionary ideals influenced Belgian society and politics.Independence and Kingdom of Belgium: Following the defeat of Napoleon, the Congress of Vienna in 1815 united the southern provinces with the northern provinces to form the United Kingdom of the Netherlands. However, tensions between the Dutch-speaking north and the French-speaking south led to the Belgian Revolution in 1830. Belgium declared independence and established a constitutional monarchy, with Leopold I as its first king.\n",
    "Industrialization and Colonialism: Throughout the 19th century, Belgium experienced rapid industrialization, particularly in coal mining and steel production. It also established a colonial empire in Africa, notably in the Congo, which was famously exploited under King Leopold II's rule.\n",
    "20th Century: Belgium was heavily impacted by both World Wars, particularly during World War I when it served as a battleground. The country was occupied by Germany during World War II. After the war, Belgium played a key role in the founding of the European Coal and Steel Community, a precursor to the European Union.\n",
    "Modern Belgium: Belgium has since become a prosperous and democratic country, known for its multiculturalism, chocolate, beer, and waffles. However, it continues to grapple with linguistic and political tensions between the Dutch-speaking Flanders region and the French-speaking Wallonia region, as well as issues related to regional autonomy and identity.'''\n",
    "\n",
    "nl_history_text1 = '''The history of the Netherlands is rich and diverse, spanning thousands of years. Here's a brief summary of key periods and events in Dutch history:\n",
    "Early Settlements: The region that is now the Netherlands has been inhabited since prehistoric times. During the Roman era, it was part of the Roman Empire's frontier region.\n",
    "Middle Ages: In the early Middle Ages, the Franks established control over the region. The Netherlands gradually emerged as a distinct entity, with the development of feudal states and the growth of trade and commerce.\n",
    "Golden Age (17th Century): The 17th century is often referred to as the Dutch Golden Age. During this time, the Netherlands experienced a period of economic prosperity, cultural flourishing, and naval dominance. The Dutch East India Company and Dutch West India Company were established, and Amsterdam became a leading financial center.\n",
    "Colonial Empire: The Dutch established colonies and trading posts around the world, including in the East Indies (present-day Indonesia), Suriname, and the Caribbean. The Dutch colonial empire was significant but eventually declined over time.\n",
    "Napoleonic Era: In the late 18th and early 19th centuries, the Netherlands fell under French control during the Napoleonic Wars. It later became part of the French Empire.\n",
    "Independence and Kingdom: The Netherlands gained independence from France in 1815 and became a kingdom under King William I. Belgium initially formed part of the Kingdom of the Netherlands but later separated in 1830.\n",
    "Industrialization and Modernization: The 19th century saw rapid industrialization and modernization in the Netherlands. The country became known for its innovations in trade, shipping, and agriculture.\n",
    "World Wars: The Netherlands remained neutral during World War I but was invaded by Nazi Germany in World War II. The country suffered under German occupation but played a role in the Allied liberation of Europe.\n",
    "Post-War Reconstruction: After World War II, the Netherlands underwent a period of reconstruction and economic recovery. It became a founding member of international organizations such as the United Nations and the European Union.\n",
    "Contemporary Era: In recent decades, the Netherlands has become known for its progressive social policies, strong economy, and commitment to environmental sustainability. It continues to be a leading global player in areas such as trade, technology, and diplomacy.\n",
    "This summary provides a broad overview of Dutch history, highlighting key moments and themes that have shaped the nation's identity and development over time.'''\n",
    "\n",
    "gr_history_text1 = '''Greek history spans thousands of years and is marked by significant contributions to Western civilization, including democracy, philosophy, art, and literature. Here's a brief summary:\n",
    "Ancient Greece: Ancient Greek civilization emerged around the 8th century BC and was comprised of city-states such as Athens, Sparta, Corinth, and Thebes. This period saw the rise of democracy in Athens, where citizens participated in governance, and the development of philosophy by figures like Socrates, Plato, and Aristotle. Greek art and architecture, exemplified by the Parthenon in Athens, also flourished during this time. The city-states often engaged in conflicts with each other, most notably the Peloponnesian War between Athens and Sparta.\n",
    "Hellenistic Period: After the conquests of Alexander the Great in the 4th century BC, Greek culture spread throughout the Mediterranean and Middle East, creating a new era known as the Hellenistic period. Greek language, art, and philosophy influenced cultures across the region, including Egypt and Persia.\n",
    "Roman Greece: Greece became part of the Roman Empire after the defeat of the Greek city-states in the 2nd century BC. During this time, Greece continued to be an important center of culture and learning, with cities like Corinth and Athens remaining influential.\n",
    "Byzantine Empire: Following the division of the Roman Empire, Greece became part of the Byzantine Empire, centered in Constantinople (modern-day Istanbul). The Byzantine period saw the spread of Christianity and the construction of numerous churches and monasteries across Greece.\n",
    "Ottoman Rule: Greece fell under Ottoman rule in the 15th century after the fall of Constantinople. The Greeks struggled for independence from Ottoman rule for centuries, culminating in the Greek War of Independence in the early 19th century.\n",
    "Modern Greece: The Greek War of Independence began in 1821 and eventually led to the establishment of the modern Greek state in 1830, although some territories, including Crete and the Ionian Islands, were not incorporated until later. The monarchy was established, and Otto of Bavaria became the first king of Greece.\n",
    "20th Century: Greece experienced political instability throughout much of the 20th century, including periods of monarchy, dictatorship, and democratic rule. Greece was occupied by Axis powers during World War II, and a brutal civil war followed the war's end. In 1974, Greece transitioned to democracy after the fall of the military junta.\n",
    "European Union and Economic Challenges: Greece joined the European Union in 1981 and adopted the euro as its currency in 2001. However, the country faced significant economic challenges in the late 2000s, leading to a sovereign debt crisis and bailout agreements with the EU and International Monetary Fund.\n",
    "Modern Greece: Today, Greece is a parliamentary republic and a member of the European Union. It remains a popular tourist destination known for its rich history, stunning landscapes, and cultural heritage. However, it continues to grapple with economic issues and challenges related to governance and social welfare. '''\n",
    "\n",
    "\n",
    "be_history_text2 = '''Belgian history is rich and complex, characterized by its strategic location in Western Europe and its cultural diversity. Here's a brief summary:\n",
    "\n",
    "Early History: The region now known as Belgium has been inhabited since prehistoric times. It was later settled by Celtic and Germanic tribes before coming under Roman rule in the first century BC. The area flourished during Roman times as part of the province of Gallia Belgica.\n",
    "\n",
    "Medieval Period: After the fall of the Roman Empire, the region was invaded and settled by various Germanic tribes. In the early Middle Ages, it became part of the Frankish Empire. During this period, the area saw the rise of powerful feudal lords and the emergence of important trading cities like Ghent, Bruges, and Antwerp.\n",
    "\n",
    "Burgundian and Habsburg Rule: In the 15th century, the Burgundian dukes gained control of much of present-day Belgium. This period saw the flourishing of arts and culture, but also increased centralization of power. The region later came under Habsburg rule as part of the Spanish and Austrian Netherlands.\n",
    "\n",
    "Dutch Independence: In the 16th and 17th centuries, the Dutch Revolt against Spanish rule led to the independence of the northern provinces of the Netherlands. However, the southern provinces, including present-day Belgium, remained under Spanish control until they were conquered by France in the late 17th century.\n",
    "\n",
    "French Rule: Belgium became part of France under Napoleon Bonaparte's rule in the early 19th century. During this time, French revolutionary ideals influenced Belgian society and politics.Independence and Kingdom of Belgium: Following the defeat of Napoleon, the Congress of Vienna in 1815 united the southern provinces with the northern provinces to form the United Kingdom of the Netherlands. However, tensions between the Dutch-speaking north and the French-speaking south led to the Belgian Revolution in 1830. Belgium declared independence and established a constitutional monarchy, with Leopold I as its first king.\n",
    "\n",
    "Industrialization and Colonialism: Throughout the 19th century, Belgium experienced rapid industrialization, particularly in coal mining and steel production. It also established a colonial empire in Africa, notably in the Congo, which was famously exploited under King Leopold II's rule.\n",
    "\n",
    "20th Century: Belgium was heavily impacted by both World Wars, particularly during World War I when it served as a battleground. The country was occupied by Germany during World War II. After the war, Belgium played a key role in the founding of the European Coal and Steel Community, a precursor to the European Union.\n",
    "\n",
    "Modern Belgium: Belgium has since become a prosperous and democratic country, known for its multiculturalism, chocolate, beer, and waffles. However, it continues to grapple with linguistic and political tensions between the Dutch-speaking Flanders region and the French-speaking Wallonia region, as well as issues related to regional autonomy and identity. '''\n",
    "\n",
    "gr_history_text2 = '''Greek history spans thousands of years and is marked by significant contributions to Western civilization, including democracy, philosophy, art, and literature. Here's a brief summary:\n",
    "\n",
    "Ancient Greece: Ancient Greek civilization emerged around the 8th century BC and was comprised of city-states such as Athens, Sparta, Corinth, and Thebes. This period saw the rise of democracy in Athens, where citizens participated in governance, and the development of philosophy by figures like Socrates, Plato, and Aristotle. Greek art and architecture, exemplified by the Parthenon in Athens, also flourished during this time. The city-states often engaged in conflicts with each other, most notably the Peloponnesian War between Athens and Sparta.\n",
    "\n",
    "Hellenistic Period: After the conquests of Alexander the Great in the 4th century BC, Greek culture spread throughout the Mediterranean and Middle East, creating a new era known as the Hellenistic period. Greek language, art, and philosophy influenced cultures across the region, including Egypt and Persia.\n",
    "\n",
    "Roman Greece: Greece became part of the Roman Empire after the defeat of the Greek city-states in the 2nd century BC. During this time, Greece continued to be an important center of culture and learning, with cities like Corinth and Athens remaining influential.\n",
    "\n",
    "Byzantine Empire: Following the division of the Roman Empire, Greece became part of the Byzantine Empire, centered in Constantinople (modern-day Istanbul). The Byzantine period saw the spread of Christianity Empire after the defeat of the Greek city-states in the 2nd century BC. During this time, Greece continued to be an important center of culture and learning, with cities like Corinth and Athens remaining influential.\n",
    "\n",
    "Byzantine Empire: Following the division of the Roman Empire, Greece became part of the Byzantine Empire, centered in Constantinople (modern-day Istanbul). The Byzantine period saw the spread of Christianity and the construction of numerous churches and monasteries across Greece.\n",
    "\n",
    "Ottoman Rule: Greece fell under Ottoman rule in the 15th century after the fall of Constantinople. The Greeks struggled for independence from Ottoman rule for centuries, culminating in the Greek War of Independence in the early 19th century.\n",
    "\n",
    "Modern Greece: The Greek War of Independence began in 1821 and eventually led to the establishment of the modern Greek state in 1830, although some territories, including Crete and the Ionian Islands, were not incorporated until later. The monarchy was established, and Otto of Bavaria became the first king of Greece.\n",
    "\n",
    "20th Century: Greece experienced political instability throughout much of the 20th century, including periods of monarchy, dictatorship, and democratic rule. Greece was occupied by Axis powers during World War II, and a brutal civil war followed the war's end. In 1974, Greece transitioned to democracy after the fall of the military junta.\n",
    "\n",
    "European Union and Economic Challenges: Greece joined the European Union in 1981 and adopted the euro as its currency in 2001. However, the country faced significant economic challenges in the late 2000s, leading to a sovereign debt crisis and bailout agreements with the EU and International Monetary Fund.\n",
    "\n",
    "Modern Greece: Today, Greece is a parliamentary republic and a member of the European Union. It remains a popular tourist destination known for its rich history, stunning landscapes, and cultural heritage. However, it continues to grapple with economic issues and challenges related to governance and social welfare. '''\n",
    "\n",
    "nl_history_text2 = '''The history of the Netherlands is rich and diverse, spanning thousands of years. Here's a brief summary of key periods and events in Dutch history:\n",
    "\n",
    "Early Settlements: The region that is now the Netherlands has been inhabited since prehistoric times. During the Roman era, it was part of the Roman Empire's frontier region.\n",
    "\n",
    "Middle Ages: In the early Middle Ages, the Franks established control over the region. The Netherlands gradually emerged as a distinct entity, with the development of feudal states and the growth of trade and commerce.\n",
    "\n",
    "Golden Age (17th Century): The 17th century is often referred to as the Dutch Golden Age. During this time, the Netherlands experienced a period of economic prosperity, cultural flourishing, and naval dominance. The Dutch East India Company and Dutch West India Company were established, and Amsterdam became a leading financial center.\n",
    "\n",
    "Colonial Empire: The Dutch established colonies and trading posts around the world, including in the East Indies (present-day Indonesia), Suriname, and the Caribbean. The Dutch colonial empire was significant but eventually declined over time.\n",
    "\n",
    "Napoleonic Era: In the late 18th and early 19th centuries, the Netherlands fell under French control during the Napoleonic Wars. It later became part of the French Empire.\n",
    "\n",
    "Independence and Kingdom: The Netherlands gained independence from France in 1815 and became a kingdom under King William I. Belgium initially formed part of the Kingdom of the Netherlands but later separated in 1830.\n",
    "\n",
    "Industrialization and Modernization: The 19th century saw rapid industrialization and modernization in the Netherlands. The country became known for its innovations in trade, shipping, and agriculture.\n",
    "\n",
    "World Wars: The Netherlands remained neutral during World War I but was invaded by Nazi Germany in World War II. The country suffered under German occupation but played a role in the Allied liberation of Europe.\n",
    "\n",
    "Post-War Reconstruction: After World War II, the Netherlands underwent a period of reconstruction and economic recovery. It became a founding member of international organizations such as the United Nations and the European Union.\n",
    "\n",
    "Contemporary Era: In recent decades, the Netherlands has become known for its progressive social policies, strong economy, and commitment to environmental sustainability. It continues to be a leading global player in areas such as trade, technology, and diplomacy.\n",
    "\n",
    "This summary provides a broad overview of Dutch history, highlighting key moments and themes that have shaped the nation's identity and development over time. '''\n",
    "\n",
    "print(f\"Number of words in the History texts:\\n(Gr, Nl, Be) = ({len(gr_history_text1.split(' '))}, {len(nl_history_text1.split(' '))}, {len(be_history_text1.split(' '))})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First page about belgium, second about grece\n",
    "filename = \"history1_be_gr_nl.pdf\"\n",
    "create_pdf([be_history_text1, gr_history_text1, nl_history_text1], filename)\n",
    "# First page about grece, second about belgium\n",
    "filename = \"history1_gr_be_nl.pdf\"\n",
    "create_pdf([gr_history_text1, be_history_text1, nl_history_text1], filename)\n",
    "\n",
    "filename = \"history2_be_gr_nl.pdf\"\n",
    "create_pdf([be_history_text2, gr_history_text2, nl_history_text2], filename)\n",
    "# First page about grece, second about belgium\n",
    "filename = \"history2_gr_be_nl.pdf\"\n",
    "create_pdf([gr_history_text2, be_history_text2, nl_history_text2], filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. Code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. Loading a pdf file in a list of docs  \n",
    "4.2. __select_sentences_range__, selects from a given text, all the sentences given between __start_index__ and __end_index__.  \n",
    "4.3. __create_pdf__ is the definition on how to use the generation of a pdf based on a list of input strings.  \n",
    "4.4. __create_pd_from_map_reduce_output__, to create a pandas __dataframe__ from the __LangChain map-reduce__ outputs  \n",
    "4.5. Code to load the Llama model (dutch or english) using __pipeline__ from __transformers__ (as present in the demo given to Casper)  \n",
    "4.6. Code to load the Llama model (dutch or english) using __HuggingFaceHub__ (errors due to modelsize >)  \n",
    "4.7. Code to upload the Llama model to Hugging Face Space (although suggested by the output error it seems not to be possible)  \n",
    "4.8. Code to upload the Llama model using __AutoModelForCausalLM__ and using the __AutoTokenizer__ and __pipeline__ from transformers wrapping everything in __HuggingFacePipeline__ that can be used by LangChain (This approach was Successfull)  \n",
    "4.9. __Cpp__ way of loading the model  \n",
    "4.10. Login to hugging face  \n",
    "4.11. Map-Reduce stress test: Map: \"Extract nth sentence of the document splits\" and Reduce: \"concatenate them into final output\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.1. Loading a pdf file in a list of docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"KI_nl.pdf\")\n",
    "docs = loader.load()\n",
    "print(f\"Number of docs: {len(docs)}\\n===============\\n\")\n",
    "print(f\"page_content of first page:\\n===========================\\n{docs[0].page_content}\\n\\n\")\n",
    "print(f\"metadata:\\n=========\\n{docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.2. __select_sentences_range__, selects from a given text, all the sentences given between start_index and end_index. \n",
    "- __select_sentences_range(text, 3, 6)__ --> will give you from the input text all the sentences from sentence 3 till sentence 6\n",
    "- __select_sentences_range(text, 3,\":\")__ --> will give you all the sentences from the text from the 3rd sentence till the end of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def select_sentences_range(text, start_index, end_index):\n",
    "    # Split the text into sentences using regex\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    \n",
    "    # Convert \":\" to the length of the sentences list\n",
    "    if end_index == \":\":\n",
    "        end_index = len(sentences)\n",
    "    \n",
    "    # Select sentences within the specified range\n",
    "    selected_sentences = sentences[start_index:end_index]\n",
    "    \n",
    "    # Join the selected sentences into a single string\n",
    "    selected_text = ' '.join(selected_sentences)\n",
    "    \n",
    "    return selected_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.3. __create_pdf__ is the definition on how to use the generation of a pdf based on a list of input strings. Each string of the list will be on its own page in the pdf document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install reportlab\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "def create_pdf(list_strings, filename):\n",
    "    # Create a canvas with letter size (8.5x11 inches)\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "\n",
    "    # Set font and font size for first page\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    \n",
    "    for string in list_strings:\n",
    "        # Draw first string on the first page\n",
    "        draw_multiline_string(c, 100, 700, string)\n",
    "\n",
    "        # Add a new page\n",
    "        c.showPage()\n",
    "\n",
    "    # Save the PDF\n",
    "    c.save()\n",
    "\n",
    "def draw_multiline_string(canvas, x, y, text, max_width=400, line_spacing=15):\n",
    "    lines = []\n",
    "    current_line = ''\n",
    "    for word in text.split():\n",
    "        if canvas.stringWidth(current_line + ' ' + word) <= max_width:\n",
    "            current_line += ' ' + word\n",
    "        else:\n",
    "            lines.append(current_line.strip())\n",
    "            current_line = word\n",
    "    lines.append(current_line.strip())\n",
    "\n",
    "    for line in lines:\n",
    "        canvas.drawString(x, y, line)\n",
    "        y -= line_spacing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet1 = \"Medewerkers gaan we verder ontwikkelen in het toepassen van de menselijke maat in onze dienstverlening. Met ondersteuning van kunstmatige intelligentie gaan we onze brieven leesbaarder en begrijpelijker maken. De benadering is van buiten naar binnen: knelpunten die onze cliënten ervaren worden in kaart gebracht op basis van verschillende vormen van (klant)onderzoek en analyses.\"\n",
    "snippet2 = \"Het programma Innovatie ondersteunt initiatieven en oplosteams in het effectief organiseren van verbetertrajecten en het bedenken van vernieuwende oplossingen, met kennis over de laatste (technologische) ontwikkelingen en trends. Zo wordt onderzocht hoe kunstmatige intelligentie (zoals ChatGPT) ingezet kan worden, bijvoorbeeld bij het herschrijven van algemene teksten in tientallen brieven om de leesbaarheid te verbeteren. Aandacht voor innovatie en design thinking draagt ook bij aan de gewenste ontwikkeling van een lerende organisatie.\"\n",
    "snippet_collection_nl = snippet1 + \"\\n\" + snippet2\n",
    "\n",
    "snippet_collection_en = '''We will further develop employees in applying the human scale in our services. With support of\n",
    "artificial intelligence we will make our letters more readable and make it more understandable. The approach is from the outside in: bottlenecks\n",
    "that our clients experience are mapped on the basis of various forms of (customer) research and analyses. The program\n",
    "Innovation supports initiatives and solution teams effectively organizing improvement processes and devising innovative ones\n",
    "solutions, with knowledge of the latest (technological) developments and trends. For example, it is investigated how artificial intelligence (such as\n",
    "ChatGPT) can be used, for example when rewriting general texts in dozens of letters to improve readability.\n",
    "Attention to innovation and design thinking also contributes to the desired results development of a learning organization.'''\n",
    "create_pdf([snippet_collection_nl], \"KI_nl.pdf\")\n",
    "create_pdf([snippet_collection_en], \"KI_en.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.4. __create_pd_from_map_reduce_output__, to create a pandas dataframe from the LangChain map-reduce outputs¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "from pathlib import Path as p\n",
    "import pandas as pd\n",
    "\n",
    "def create_pd_from_map_reduce_output(map_reduce_outputs):\n",
    "    data_folder = p.cwd() \n",
    "    p(data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    final_mp_data = []\n",
    "    for doc, out in zip(map_reduce_outputs[\"input_documents\"], map_reduce_outputs[\"intermediate_steps\"]):\n",
    "        output = {}\n",
    "        output[\"file_name\"] = p(doc.metadata[\"source\"]).stem\n",
    "        output[\"file_type\"] = p(doc.metadata[\"source\"]).suffix\n",
    "        output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "        output[\"chunks\"] = doc.page_content\n",
    "        output[\"concise_summary\"] = out\n",
    "        final_mp_data.append(output)\n",
    "\n",
    "    pdf_mp_summary = pd.DataFrame.from_dict(final_mp_data)\n",
    "    pdf_mp_summary = pdf_mp_summary.sort_values(by=[\"file_name\", \"page_number\"])  # sorting the dataframe by filename and page_number\n",
    "    pdf_mp_summary.reset_index(inplace=True, drop=True)\n",
    "    pdf_mp_summary\n",
    "    return pdf_mp_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that prints the output in a human readable format \n",
    "for i in range(len(pdf_mp_summary)):\n",
    "   print(f\"\\n ========================================\\n{pdf_mp_summary.iloc[i]['chunks']} \\n+++++++++++++++++++++++\\n{pdf_mp_summary.iloc[i]['concise_summary']}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.5. Code to load the Llama model (dutch or english) using the __pipeline__ from __transformers__ (as present in the demo given to Casper)\n",
    "- set in the below cell the parameter __model_chosen_id__ either to __=1__ (english model) or to __=2__ (dutch model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langdetect\n",
    "import re\n",
    "import time\n",
    "from transformers import pipeline, Conversation, AutoTokenizer\n",
    "from langdetect import detect\n",
    "\n",
    "# choose your model here by setting model_chosen_id equal to 1 or 2\n",
    "model_chosen_id = 1\n",
    "model_name_options = {\n",
    "    1: \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    2: \"BramVanroy/Llama-2-13b-chat-dutch\"\n",
    "}\n",
    "model_chosen = model_name_options[model_chosen_id]\n",
    "\n",
    "my_config = {'model_name': model_chosen, 'do_sample': True, 'temperature': 0.1, 'repetition_penalty': 1.1, 'max_new_tokens': 500, }\n",
    "print(f\"Selected model: {my_config['model_name']}\")\n",
    "print(f\"Parameters are: {my_config}\")\n",
    "\n",
    "def count_words(text):\n",
    "    # Use a simple regular expression to count words\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return len(words)\n",
    "\n",
    "def generate_with_llama_chat(my_config):    \n",
    "    # get the parameters from the config dict\n",
    "    do_sample = my_config.get('do_sample', True)\n",
    "    temperature = my_config.get('temperature', 0.1)\n",
    "    repetition_penalty = my_config.get('repetition_penalty', 1.1)\n",
    "    max_new_tokens = my_config.get('max_new_tokens', 500)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = my_config['model_name']\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    \n",
    "    # Language code for Dutch\n",
    "    #lang_code = \"nl_XX\"\n",
    "    #forced_bos_token_id = tokenizer.lang_code_to_id[\"nl_XX\"] # Error lang_code_to_id not know\n",
    "    \n",
    "    #potential usful parameters to tweak: ,\"do_sample\": True, \"max_lengt\n",
    "    chatbot = pipeline(\"conversational\",model=model, \n",
    "                       tokenizer=tokenizer,\n",
    "                       do_sample=do_sample, \n",
    "                       temperature=temperature, \n",
    "                       repetition_penalty=repetition_penalty,\n",
    "                       #max_length=2000,\n",
    "                       max_new_tokens=max_new_tokens, \n",
    "                       model_kwargs={\"device_map\": \"auto\",\"load_in_8bit\": True})  #, \"src_lang\": \"en\", \"tgt_lang\": \"nl\"})  does not work!\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Loading the model: {elapsed_time} seconds\")\n",
    "    return chatbot\n",
    "    \n",
    "def get_answer(chatbot, input_text):\n",
    "    start_time = time.time()\n",
    "    print(f\"Processing the input\\n {input_text}\\n\")\n",
    "    print('Processing the answer....')\n",
    "    conversation = Conversation(input_text)\n",
    "    print(f\"Conversation(input_text): {conversation}\")\n",
    "    output = (chatbot(conversation))[1]['content']\n",
    "    output_language = detect(output)\n",
    "    print(f\"{output}\\n\")\n",
    "    print(f\"output language detected is {output_language}\\n\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Answered in {elapsed_time:.1f} seconds, Nr generated words: {count_words(output)}\\n\")\n",
    "\n",
    "    # Perform translation to dutch (catch in case it is needed (prompt engineering does not always works)\n",
    "    if output_language == 'en':\n",
    "        print(\"----------------------------------------------------\")\n",
    "        print(\"Need extra time to make the translation to Dutch....\")\n",
    "        start_time = time.time()\n",
    "        conversation = Conversation(f\"Translate the following text to Dutch: {output}\")\n",
    "        output = (chatbot(conversation))[1]['content']\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"translated output is: {output}\\n\")\n",
    "        print(f\"Translation time: {elapsed_time:.1f}, Nr generated words: {count_words(output)}\")\n",
    "\n",
    "\n",
    "chatbot = generate_with_llama_chat(my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet1 = \"Medewerkers gaan we verder ontwikkelen in het toepassen van de menselijke maat in onze dienstverlening. Met ondersteuning van kunstmatige intelligentie gaan we onze brieven leesbaarder en begrijpelijker maken. De benadering is van buiten naar binnen: knelpunten die onze cliënten ervaren worden in kaart gebracht op basis van verschillende vormen van (klant)onderzoek en analyses.\"\n",
    "snippet2 = \"Het programma Innovatie ondersteunt initiatieven en oplosteams in het effectief organiseren van verbetertrajecten en het bedenken van vernieuwende oplossingen, met kennis over de laatste (technologische) ontwikkelingen en trends. Zo wordt onderzocht hoe kunstmatige intelligentie (zoals ChatGPT) ingezet kan worden, bijvoorbeeld bij het herschrijven van algemene teksten in tientallen brieven om de leesbaarheid te verbeteren. Aandacht voor innovatie en design thinking draagt ook bij aan de gewenste ontwikkeling van een lerende organisatie.\"\n",
    "snippet_collection = snippet1 + \"\\n\" + snippet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_dict_snippets = {\n",
    "    1: \"Wat wordt er over 'kunstmatige intelligentie' besproken?\",\n",
    "    2: \"Geef me een samenvatting van het document.\",\n",
    "    #3: \"Geef me een samenvatting van 'snippet2'.\",\n",
    "    #4: \"Geef me een samenvatting van 'snippet1' and 'snippet2'.\",\n",
    "    #5: \"Wat is de tekst die overeenkomt met 'snippet1'?\",\n",
    "    #6: \"Wat is de tekst die overeenkomt met 'snippet2'?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = f\"{question_dict_snippets[1]} in de volgende text tussen quotations: '{snippet_collection}'\"\n",
    "print(input_text)\n",
    "output = get_answer(chatbot, input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.6. Code to load the Llama model (dutch or english) using __HuggingFaceHub__: (Does not work)\n",
    "- __Error:__\n",
    "  The model BramVanroy/Llama-2-13b-chat-dutch is too large to be loaded automatically (26GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\n",
    "\n",
    "  - There are two options to circumvent this issue:\n",
    "    1) Using Hugging Face Spaces\n",
    "    2) Using Inference Endpoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "christos_hf_token = \"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\"\n",
    "polpo_hf_token = \"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\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = polpo_hf_token\n",
    "\n",
    "# Verify if the environment variable contains the right content\n",
    "print(os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"BramVanroy/Llama-2-13b-chat-dutch\", model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "print(llm_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.7. Code to uploading the model to my Hugging Face __Space__ (Does not work)\n",
    "__Error:__ ```usage: huggingface-cli <command> [<args>]\n",
    "huggingface-cli: error: argument {env,login,whoami,logout,repo,upload,download,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache}: invalid choice: 'space' (choose from 'env', 'login', 'whoami', 'logout', 'repo', 'upload', 'download', 'lfs-enable-largefiles', 'lfs-multipart-upload', 'scan-cache', 'delete-cache')```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install huggingface_hub\n",
    "#huggingface-cli login\n",
    "!huggingface-cli space push model my_BramVanroy_Dutch_model BramVanroy/Llama-2-13b-chat-dutch --organization polpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli upload model my_BramVanroy_Dutch_model BramVanroy/Llama-2-13b-chat-dutch --organization polpo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.8. Loading the model using __AutoModelForCausalLM__ and using the AutoTokenizer and pipeline from transformers wrapping everything in __LangChain__ pipeline (called __HuggingFacePipeline__)\n",
    "4.8.1. llm that can be used with prompting (This is prerequisite to run the rest from 4.8.2-4.8.5)  \n",
    "4.8.2. General question-prompt about the EEG without input context  \n",
    "4.8.3. Prompt asking to summarize the input text with __LangChain Stuff__ method  \n",
    "4.8.4. Prompt asking to summarize the input text with __LangChain Map-Reduce__ method  \n",
    "4.8.5. Prompt asking to summarize the input text with __LangChain Refine__ method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.8.1. llm that can be used with prompting\n",
    "- !pip install transformers\n",
    "- !pip install accelerate\n",
    "- !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, Conversation, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "#1: \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "#2: \"BramVanroy/Llama-2-13b-chat-dutch\"\n",
    "my_config = {'model_name': \"BramVanroy/Llama-2-13b-chat-dutch\", #\"./Bram\", #BramVanroy/Llama-2-13b-chat-dutch\", \n",
    "             'do_sample': True, 'temperature': 0.1, \n",
    "             'repetition_penalty': 1.1, 'max_new_tokens': 500, }\n",
    "\n",
    "print(f\"Selected model: {my_config['model_name']}\")\n",
    "print(f\"Parameters are: {my_config}\")\n",
    "\n",
    "def generate_with_llama_chat(my_config):\n",
    "    print('tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(my_config['model_name'])\n",
    "    print('causal')\n",
    "    model = AutoModelForCausalLM.from_pretrained(my_config['model_name'])\n",
    "    print('Pipeline')\n",
    "    chatbot = pipeline(\"text-generation\",model=my_config['model_name'], \n",
    "                       tokenizer=tokenizer,\n",
    "                       do_sample=my_config['do_sample'], \n",
    "                       temperature=my_config['temperature'], \n",
    "                       repetition_penalty=my_config['repetition_penalty'],\n",
    "                       #max_length=my_config['max_length'],\n",
    "                       max_new_tokens=my_config['max_new_tokens'], \n",
    "                       model_kwargs={\"device_map\": \"auto\",\"load_in_8bit\": True})\n",
    "    return chatbot\n",
    "\n",
    "llama_chat = generate_with_llama_chat(my_config)\n",
    "\n",
    "# Set up callback manager to print output word by word\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llama_chat, callback_manager=callback_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.8.2. General question-prompt about the EEG without input context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.8.3. Prompt asking to summarize the input text with __LangChain -> Stuff__ method\n",
    "- You can specify your own prompt_template_ (A,B,C, ...) but be sure to also change the code below so that the correct template is executed:\n",
    "- ```prompt = PromptTemplate(template=prompt_templateB, input_variables=[\"text\"])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "prompt_template_A = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "  \"\"\"\n",
    "\n",
    "prompt_template_B = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
    "              ```{text}```\n",
    "              HELPFULL SUMMARY:\n",
    "  \"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template_B, input_variables=[\"text\"])\n",
    "\n",
    "#1) stuff_chain only works when entire document fits into the n_ctx (number of context tokens)\n",
    "stuff_chain = load_summarize_chain(llm= llm, chain_type=\"stuff\",prompt=prompt)\n",
    "\n",
    "loader = PyPDFLoader(\"KI_en.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "try:\n",
    "    output = stuff_chain.invoke(pages)\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(\n",
    "        \"The code failed since it won't be able to run inference on such a huge context and throws this exception: \",\n",
    "        e,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.8.4. Prompt asking to summarize the input text with __LangChain -> Map-Reduce__ method\n",
    "- Result: As can be seen in the table below the concise_summary for the first document (Greek History) gives gibberish output, speaking about what a summary is but actually not giving the summary about the first input chunk. The second (Belgium History) and third (Dutch History) chunk, do give descent results but i have the feeling the last summary is truncated.\n",
    "\n",
    " \n",
    "  |index | file_name             | file_type\t| page_number\t|         chunks                                    | concise_summary                                 |\n",
    "  |------|-----------------------|--------------|---------------|---------------------------------------------------|-------------------------------------------------|\n",
    "  | 0\t | history2_gr_be_nl\t | .pdf         |     0\t        | Greek history spans thousands of years and is ... | Summaries are written in complete sentences t...|\n",
    "  | 1    | history2_gr_be_nl\t | .pdf\t        |     1\t        | Belgian history is rich and complex, character... | Summaries of Belgian History.\\n\\nHere are som...|\n",
    "  | 2\t | history2_gr_be_nl\t | .pdf         |     2         | The history of the Netherlands is rich and div... | Here are three summaries based on the provid... |\n",
    "\n",
    "- I also tried to add the Greek history another time to the document, (so full pdf has 4 pages, Gr, Nl, Be and Gr history snippet) in the hope to see a summary of the last Gr chunk. But i got the same gibberish output as on the pdf with 3 pages.\n",
    "- Reading in the be_gr_nl history pdf instead of the gr_be_nl one gives now for the second chunk (Gr History) the gibberish answer.\n",
    "- CONCLUSION1: This means that there is something wrong with the Greek history input that leads to a gibberish answer.\n",
    "- CONCLUSION2: ```UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset``` Some research is needed in order to benefit from the parallel offload of the map process. All the subdocuments could in principle be processed in parallel and the reduce step could collect the final output from the reduce step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "#from datasets import Dataset\n",
    "\n",
    "loader = PyPDFLoader(\"history1_gr_be_nl.pdf\")\n",
    "#loader = PyPDFLoader(\"history1_be_gr_nl.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# To create a list of strings from each doc in docs you can apply the following peice of code\n",
    "#split_text = [doc.page_content for doc in docs]\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please make summaries. Do not summarize the document when there are no full sentences in the document. \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Reduce\n",
    "#\"\"\"The following is set of summaries:\n",
    "#{docs}\n",
    "#Take the above summaries and combine them into a final summary \n",
    "#Helpful Answer: \n",
    "#\"\"\"\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_maps}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_maps\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4096\n",
    ")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Combining summaries by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=True\n",
    ")\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=800, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# For .txt files you can use the following text splitter\n",
    "#split_text = text_splitter.split_text(text_input)\n",
    "#print(f\"Number of splits= {len(split_text)}\")\n",
    "\n",
    "# For pdf documents you can use the following documents splitter\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"Number of splits= {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "map_reduce_output1 = map_reduce_chain.invoke(split_docs)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\n\\nElapsed time:  {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the create_pd_from_map_reduce_output from section 4.4\n",
    "pdf_mp_summary = create_pd_from_map_reduce_output(map_reduce_output1)\n",
    "\n",
    "# code that prints the output in a human readable format \n",
    "for i in range(len(pdf_mp_summary)):\n",
    "   print(f\"\\n ========================================\\n{pdf_mp_summary.iloc[i]['chunks']} \\n+++++++++++++++++++++++\\n{pdf_mp_summary.iloc[i]['concise_summary']}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_mp_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_reduce_output1['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.8.5. Prompt asking to summarize the input text with __LangChain -> Refine__ method\n",
    "- (https://python.langchain.com/docs/use_cases/summarization)\n",
    "The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be tested/adjusted to my needs:\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"history2_be_gr_nl.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "question_prompt_template = \"\"\"\n",
    "                  Please provide a summary of the following text.\n",
    "                  TEXT: {text}\n",
    "                  SUMMARY:\n",
    "                  \"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "refine_prompt_template = \"\"\"\n",
    "              Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "              \"\"\"\n",
    "\n",
    "refine_prompt = PromptTemplate(\n",
    "    template=refine_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "refine_chain = load_summarize_chain(\n",
    "    llm= llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=question_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=800, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# For .txt files you can use the following text splitter\n",
    "#split_text = text_splitter.split_text(text_input)\n",
    "#print(f\"Number of splits= {len(split_text)}\")\n",
    "\n",
    "# For pdf documents you can use the following documents splitter\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"Number of splits= {len(split_docs)}\")\n",
    "\n",
    "refine_outputs = refine_chain.invoke({\"input_documents\": split_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the create_pd_from_map_reduce_output from section 4.4\n",
    "pdf_mp_summary = create_pd_from_map_reduce_output(refine_outputs)\n",
    "\n",
    "# code that prints the output in a human readable format \n",
    "for i in range(len(pdf_mp_summary)):\n",
    "   print(f\"\\n ========================================\\n{pdf_mp_summary.iloc[i]['chunks']} \\n+++++++++++++++++++++++\\n{pdf_mp_summary.iloc[i]['concise_summary']}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be tested/adjusted to my needs:\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a text\\n\"\n",
    "    \"We have extracted the first summary from a documnet: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary.\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=200, chunk_overlap=0\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "result = chain.invoke({\"input_documents\": split_docs}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.9. LlamaCpp:  \n",
    "Subsections are:  \n",
    "4.9.1. Llama - Cpp: __llm_ instance created with __gguf-type__ file   \n",
    "4.9.2. Llama - Cpp: __Stuff Chain__ (entire input document fits into the llm, no need to split input text)  \n",
    "4.9.3. Llama Dutch Cpp: llm instance from __locally saved__ model under /workspace/Bram/  \n",
    "\n",
    "__Parameter explanation:__\n",
    "  - __temperature=0.1__,   _(default = 0.8)_\n",
    "    <br>Higher temperatures lead to more diverse and varied outputs, as tokens with lower probabilities are more likely to be sampled. Conversely, lower temperatures produce more conservative and deterministic outputs, favoring tokens with higher probabilities.\n",
    "  - __max_tokens=100__,    _(default = 256)_ \n",
    "  <br>The maximum number of tokens considered to generate an answer.After generating the specified number of tokens, the model stops generating additional tokens, ensuring that the output remains within the desired length. It helps prevent the model from generating overly verbose or irrelevant responses.\n",
    "  - __n_ctx=4096__,         _(default = 512)_\n",
    "    <br>It determines the size of the context window or the maximum number of tokens that the model considers when generating or predicting the next token in a sequence. It controls how much past information the model can consider when making predictions.\n",
    "  - __repeat_penalty=1.5__,  _(default =1.1)_\n",
    "    <br>Values between 1.1 and 1.5 are often used to apply a moderate penalty to repeated tokens. Values between 1.5 and 2.0 impose a stronger penalty on repeated tokens, leading to even greater diversity in the generated text. This can be useful when generating longer texts or when minimizing redundancy is a priority. Values greater than 2.0 apply a very strong penalty to repeated tokens, resulting in highly diverse output with minimal repetition. However, setting the repeat_penalty too high may risk reducing the coherence or naturalness of the generated text.\n",
    "  - __last_n_tokens_size = 20__  _(default = 64)_\n",
    "    <br>A larger value means that the model will consider a longer context when determining if a token should be penalized for repetition or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.9.1. Llama - Cpp: llm instance created with gguf-type file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./llama-2-13b.Q6_K.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False,  # Verbose is required to pass to the callback manager\n",
    "    temperature=0.1, \n",
    "    max_tokens=50,\n",
    "    n_ctx=4096,\n",
    "    repeat_penalty=1.2,\n",
    "    last_n_tokens_size = 96\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.9.2. Llama Cpp: Stuff Chain (entire input document fits into the llm, no need to split input text):\n",
    "- When the __max_tokens__ parameter of the llm instance > nr of input tokens of the input text --> Full input-text is given as the output (no summarization is made)\n",
    "- When __max_tokens__ < nr of input tokens --> answer = input text truncated at the number of tokens specified by max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "  \"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "#1) stuff_chain only works when entire document fits into the n_ctx (number of context tokens)\n",
    "stuff_chain = load_summarize_chain(llm= llm, chain_type=\"stuff\",prompt=prompt)\n",
    "\n",
    "loader = PyPDFLoader(\"KI_en.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "try:\n",
    "    output = stuff_chain.invoke(pages)\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(\n",
    "        \"The code failed since it won't be able to run inference on such a huge context and throws this exception: \",\n",
    "        e,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4.9.3. Llama Dutch Cpp: llm instance from __locally saved model__ under /workspace/Bram/\n",
    "- __Error__: gguf_init_from_file: invalid magic characters ''\n",
    "llama_model_load: error loading model: llama_model_loader: failed to load model from /workspace/Bram/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/workspace/Bram/\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False,  # Verbose is required to pass to the callback manager\n",
    "    temperature=0.1, \n",
    "    max_tokens=200,\n",
    "    n_ctx=4096,\n",
    "    repeat_penalty=1.2,\n",
    "    last_n_tokens_size = 96\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.10. Login to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# add this token in the Token user-input: \"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\"\n",
    "#!pip install huggingface_hub\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\") # christos token\n",
    "\n",
    "# developers@polpo.nl passwd: Polpoai2024@\n",
    "#polpo_hf_token=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add this token in the Token user-input: \"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\"\n",
    "#!pip install huggingface_hub\n",
    "from huggingface_hub import login\n",
    "login(token=polpo_hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4.11. Extract nth sentence of the document splits and concatenate them into final output\n",
    "- This was a test to see if Map-Reduce is working. SO the map is extracting the n-th sentence from each subdocument. And the Reduce was to concatenate all those sentences together to one final output. This idea was not fully tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"LandelijkDekkendNetwerk.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Map\n",
    "map_template = \"\"\"Extract the second sentence from each document split:\n",
    "{docs}\n",
    "\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Reduce\n",
    "#\"\"\"The following is set of summaries:\n",
    "#{docs}\n",
    "#Take the above summaries and combine them into a final summary \n",
    "#Helpful Answer: \n",
    "#\"\"\"\n",
    "reduce_template = \"\"\"Concatenate sentences togeter into a single text:\n",
    "{doc_maps}\n",
    "\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_maps\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=512,\n",
    ")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Combining summaries by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=512, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# For .txt files\n",
    "# text_splitter.split_text(text_input)\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Instructions on how to setup a __Streamlit Based Demo in Runpod__  \n",
    "5.1. Installs streamlit dependencies  \n",
    "5.2. Install nodejs and npx using the command line interpretor  \n",
    "5.3. Writes the streamlit code for the app  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.1. Installs streamlit dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install streamlit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5.2. Install nodejs and npx using the command line interpretor  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "__In order to be able to tunnel the streamlit app you need to install nodejs and npx.__ I the following you will see how to do that.\n",
    "To open the commandline terminal, open a new tab (this will open a tab called Launcher) and choose under \"Other\" --> \"Terminal\" and enter the following commands:\n",
    "- root@49364ccf316b:/workspace# ```apt update```\n",
    "- root@49364ccf316b:/workspace# ```apt install nodejs npm ```  \n",
    "\n",
    "To check if nodejs is installed you can do (you should get the version number displayed in the output):\n",
    "- root@49364ccf316b:/workspace# ```node --version```\n",
    "- root@49364ccf316b:/workspace# ```npm --version```\n",
    "\n",
    "Installing localtunnel using npx:\n",
    "- root@49364ccf316b:/workspace# ```npm install -g localtunnel```\n",
    "\n",
    "You can try to run the localtunnel by the following command:\n",
    "- root@49364ccf316b:/workspace# ```npx localtunnel --port 8501```  \n",
    "- click the link http link that is provided to you in the output\n",
    "  \n",
    "If the new webpage opens and a password is asked, you can retrieve the passwd from the commandline using the following command:\n",
    "- root@49364ccf316b:/workspace# ```curl https://loca.lt/mytunnelpassword && echo```\n",
    "- or try this one: ```46.227.68.162```\n",
    "\n",
    "To see if any streamlit app is still running:\n",
    "- root@49364ccf316b:/workspace# ```ps aux | grep streamlit```\n",
    "  | USER    | PID  | %CPU | %MEM | VSZ (KB) | RSS (KB) | TTY   | STAT | TIME     | Duration | COMMAND         |    APPLICTION             | CMD | NAME                      |\n",
    "  |---------|------|------|------|----------|----------|-------|------|----------|----------|-----------------|---------------------------|-----|---------------------------|\n",
    "  | root    | 5422 | 0.3  | 0.0  | 10598864 | 494092   |   ?   |  Sl  |  10:23   | 0:06     | /usr/bin/python | /usr/local/bin/streamlit  | run | LlamaDutchDemoApp.py      |\n",
    "  | root    | 5615 | 0.1  | 0.0  | 3121308  | 131696   |   ?   |  Sl  |  10:27   | 0:02     | /usr/bin/python | /usr/local/bin/streamlit  | run | LlamaDutchDemoApp.py      |\n",
    "  | root    | 6692 | 0.0  | 0.0  |  3840    | 1928     | pts/1 |  S+  |  10:55   | 0:00     | grep --color=auto|  streamlit               |     |                           |\n",
    "  \n",
    "To kill any running streamlit app:\n",
    "- root@49364ccf316b:/workspace# ```kill -9 <PID>``` (replace the <PID> with the corresponding number from the __ps aux__ table)  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Writes the streamlit code for the app & Run it\n",
    "The execution of this cell, creates the __LlamaDutchDemoApp.py__ file on the fly in the working directory  \n",
    "5.3.1. __LlamaDutchDemoApp.py__: Create the LlamaDutchDemoApp.py code (No LangChain)  \n",
    "   > 5.3.1.1. Prompt stuff  \n",
    "   > 5.3.1.2. Demo where the operator can change the promt question (code generator App.py)  \n",
    "   > 5.3.1.3. Demo with predefined promt (More for Customer exposure) (code generator App.py)  \n",
    "\n",
    "5.3.2. __Run__: the app  \n",
    "   > 5.3.2.1. Using __tunneling__  \n",
    "   > 5.3.2.2. Using __ngrok__  \n",
    "\n",
    "5.3.3. __LlamaDutchDemoApp.py__ with LangChain  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5.3.1. __LlamaDutchDemoApp.py__:Create the LlamaDutchDemoApp.py code \n",
    "> 5.3.1.1. Prompt stuff  \n",
    "> 5.3.1.2. Demo where the operator can change the promt question (code generator App.py)  \n",
    "> 5.3.1.3. Demo with predefined promt (More for Customer exposure) (code generator App.py)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1.1. Prompt stuff: Example on how to use the prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.  Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.  If you don't know the answer to a question, please don't share false information.\n",
    "«/SYS»\n",
    "\n",
    "Write a summary  in a couple of sentences for the following article. \n",
    "\n",
    "Article:  \"The history of the Netherlands is rich and diverse, spanning thousands of years.  Early Settlements: The region that is now the Netherlands has been inhabited since prehistoric times. During the Roman era, it was part of the Roman Empire's frontier region. Middle Ages: In the early Middle Ages, the Franks established control over the region. The Netherlands gradually emerged as a distinct entity, with the development of feudal states and the growth of trade and commerce. Golden Age (17th Century): The 17th century is often referred to as the Dutch Golden Age. During this time, the Netherlands experienced a period of economic prosperity, cultural flourishing, and naval dominance. The Dutch East India Company and Dutch West India Company were established, and Amsterdam became a leading financial center. Colonial Empire: The Dutch established colonies and trading posts around the world, including in the East Indies (present-day Indonesia), Suriname, and the Caribbean. The Dutch colonial empire was significant but eventually declined over time. Napoleonic Era: In the late 18th and early 19th centuries, the Netherlands fell under French control during the Napoleonic Wars. It later became part of the French Empire. Independence and Kingdom: The Netherlands gained independence from France in 1815 and became a kingdom under King William I. Belgium initially formed part of the Kingdom of the Netherlands but later separated in 1830. Industrialization and Modernization: The 19th century saw rapid industrialization and modernization in the Netherlands. The country became known for its innovations in trade, shipping, and agriculture. World Wars: The Netherlands remained neutral during World War I but was invaded by Nazi Germany in World War II. The country suffered under German occupation but played a role in the Allied liberation of Europe. Post-War Reconstruction: After World War II, the Netherlands underwent a period of reconstruction and economic recovery. It became a founding member of international organizations such as the United Nations and the European Union. Contemporary Era: In recent decades, the Netherlands has become known for its progressive social policies, strong economy, and commitment to environmental sustainability. It continues to be a leading global player in areas such as trade, technology, and diplomacy. This summary provides a broad overview of Dutch history, highlighting key moments and themes that have shaped the nation's identity and development over time.\" [/INST]\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  \n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.  \n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.  \n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "«/SYS» \n",
    "Write a summary  in a couple of sentences for the following article.\n",
    "\n",
    "Article:  \\n{BODY}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_nl = \"The history of the Netherlands can be traced back to ancient civilizations like the Romans who ruled the area before becoming an independent state after gaining freedom from Napoleon Bonaparte at the end of WWI. Since then they have gone on to form one of most powerful economies globally due their focus on technological advancements along with other factors which helped shape what we see today -a thriving society full potential opportunities waiting those willing take risks!\"\n",
    "text_gr = \"Greek history spans thousands of years and is marked by significant contributions to Western civilization, including democracy, philosophy, art, and literature. Ancient Greece: Ancient Greek civilization emerged around the 8th century BC and was comprised of city-states such as Athens, Sparta, Corinth, and Thebes. This period saw the rise of democracy in Athens, where citizens participated in governance, and the development of philosophy by figures like Socrates, Plato, and Aristotle. Greek art and architecture, exemplified by the Parthenon in Athens, also flourished during this time. The city-states often engaged in conflicts with each other, most notably the Peloponnesian War between Athens and Sparta. Hellenistic Period: After the conquests of Alexander the Great in the 4th century BC, Greek culture spread throughout the Mediterranean and Middle East, creating a new era known as the Hellenistic period. Greek language, art, and philosophy influenced cultures across the region, including Egypt and Persia. Roman Greece: Greece became part of the Roman Empire after the defeat of the Greek city-states in the 2nd century BC. During this time, Greece continued to be an important center of culture and learning, with cities like Corinth and Athens remaining influential. Byzantine Empire: Following the division of the Roman Empire, Greece became part of the Byzantine Empire, centered in Constantinople (modern-day Istanbul). The Byzantine period saw the spread of Christianity and the construction of numerous churches and monasteries across Greece. Ottoman Rule: Greece fell under Ottoman rule in the 15th century after the fall of Constantinople. The Greeks struggled for independence from Ottoman rule for centuries, culminating in the Greek War of Independence in the early 19th century. Modern Greece: The Greek War of Independence began in 1821 and eventually led to the establishment of the modern Greek state in 1830, although some territories, including Crete and the Ionian Islands, were not incorporated until later. The monarchy was established, and Otto of Bavaria became the first king of Greece. 20th Century: Greece experienced political instability throughout much of the 20th century, including periods of monarchy, dictatorship, and democratic rule. Greece was occupied by Axis powers during World War II, and a brutal civil war followed the war's end. In 1974, Greece transitioned to democracy after the fall of the military junta. European Union and Economic Challenges: Greece joined the European Union in 1981 and adopted the euro as its currency in 2001. However, the country faced significant economic challenges in the late 2000s, leading to a sovereign debt crisis and bailout agreements with the EU and International Monetary Fund. Modern Greece: Today, Greece is a parliamentary republic and a member of the European Union. It remains a popular tourist destination known for its rich history, stunning landscapes, and cultural heritage. However, it continues to grapple with economic issues and challenges related to goverment.\"\n",
    "text_be = \"Belgian history is rich and complex, characterized by its strategic location in Western Europe and its cultural diversity. Early History: The region now known as Belgium has been inhabited since prehistoric times. It was later settled by Celtic and Germanic tribes before coming under Roman rule in the first century BC. The area flourished during Roman times as part of the province of Gallia Belgica. Medieval Period: After the fall of the Roman Empire, the region was invaded and settled by various Germanic tribes. In the early Middle Ages, it became part of the Frankish Empire. During this period, the area saw the rise of powerful feudal lords and the emergence of important trading cities like Ghent, Bruges, and Antwerp. Burgundian and Habsburg Rule: In the 15th century, the Burgundian dukes gained control of much of present-day Belgium. This period saw the flourishing of arts and culture, but also increased centralization of power. The region later came under Habsburg rule as part of the Spanish and Austrian Netherlands. Dutch Independence: In the 16th and 17th centuries, the Dutch Revolt against Spanish rule led to the independence of the northern provinces of the Netherlands. However, the southern provinces, including present-day Belgium, remained under Spanish control until they were conquered by France in the late 17th century. French Rule: Belgium became part of France under Napoleon Bonaparte's rule in the early 19th century. During this time, French revolutionary ideals influenced Belgian society and politics.Independence and Kingdom of Belgium: Following the defeat of Napoleon, the Congress of Vienna in 1815 united the southern provinces with the northern provinces to form the United Kingdom of the Netherlands. However, tensions between the Dutch-speaking north and the French-speaking south led to the Belgian Revolution in 1830. Belgium declared independence and established a constitutional monarchy, with Leopold I as its first king. Industrialization and Colonialism: Throughout the 19th century, Belgium experienced rapid industrialization, particularly in coal mining and steel production. It also established a colonial empire in Africa, notably in the Congo, which was famously exploited under King Leopold II's rule. 20th Century: Belgium was heavily impacted by both World Wars, particularly during World War I when it served as a battleground. The country was occupied by Germany during World War II. After the war, Belgium played a key role in the founding of the European Coal and Steel Community, a precursor to the European Union. Modern Belgium: Belgium has since become a prosperous and democratic country, known for its multiculturalism, chocolate, beer, and waffles. However, it continues to grapple with linguistic and political tensions between the Dutch-speaking Flanders region and the French-speaking Wallonia region, as well as issues related to regional autonomy and identity.\"\n",
    "print(prompt.format(BODY=text_nl))\n",
    "print('============================')\n",
    "print(prompt.format(BODY=text_gr))\n",
    "print('============================')\n",
    "print(prompt.format(BODY=text_be))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "####  5.3.1.2. Demo where the operator can change the promt question (code generator App.py)   \n",
    "This demo is more meant for an operator that wants to play with different prompts:\n",
    "This demo contains 2 input fields:  \n",
    "- text to summarize  \n",
    "- the question to be added to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting App.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile App.py\n",
    "# This is just a very small test app to see if streamlit in runpod is actually working or not.\n",
    "import re\n",
    "import time\n",
    "import streamlit as st\n",
    "from transformers import pipeline, Conversation, AutoTokenizer\n",
    "\n",
    "#my_config = {'do_sample': True, 'temperature': 0.1, \n",
    "#             'repetition_penalty': 1.1, 'max_new_tokens': 500}\n",
    "#\"BramVanroy/Llama-2-13b-chat-dutch\"\n",
    "prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  \n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.  \n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.  \n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "«/SYS» \n",
    "{PersonalPrompt} \n",
    "\n",
    "Article:  \"{BODY}\" [/INST]\"\"\"\n",
    "\n",
    "model_name = \"BramVanroy/Llama-2-13b-chat-dutch\"\n",
    "def generate_with_llama_dutch(model_name):\n",
    "    print(\"Into generate_with_llama_dutch\")\n",
    "    time_load_model_start = time.time()\n",
    "    \n",
    "    # Load the model and tokenizer outside of the functions\n",
    "    llm = pipeline(\"conversational\", \n",
    "                   model=model_name, \n",
    "                   tokenizer=AutoTokenizer.from_pretrained(model_name),\n",
    "                   do_sample=True, \n",
    "                   temperature=0.1, \n",
    "                   repetition_penalty=1.3,\n",
    "                   max_new_tokens=512,\n",
    "                   model_kwargs={\"device_map\": \"auto\",\"load_in_8bit\": True}\n",
    "                  )\n",
    "    time_load_model_end = time.time()\n",
    "    loading_time = time_load_model_end - time_load_model_start\n",
    "    print(f\"Elapsed time to load the model: {loading_time:.2f} sec\")\n",
    "    return llm, loading_time  \n",
    "\n",
    "def get_answer(chatbot, input_text):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        print(f\"Processing the input\\n {input_text}\\n\")\n",
    "        print('Processing the answer....')\n",
    "        \n",
    "        conversation = Conversation(input_text)\n",
    "        print(f\"Conversation is: {conversation}\\n\")\n",
    "        output = (chatbot(conversation))[1]['content']\n",
    "        print(f\"Output is:\\n==========\\n {output}\\n\")\n",
    "        # Calculate elapsed time and add it to the output\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        output += f\"\\n  ---> Answered in {elapsed_time:.1f} seconds, Nr generated words: {count_words(output)}\"\n",
    "        \n",
    "        return output\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error processing input: {e}\")\n",
    "        return None\n",
    "        \n",
    "    \n",
    "def count_words(text):\n",
    "    # Use a simple regular expression to count words\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return len(words)\n",
    "\n",
    "if \"model\" not in st.session_state.keys():\n",
    "    st.write(f\"Loading {model_name}..., Please wait.\" )\n",
    "    # Initialize the model with the default option\n",
    "    #st.session_state[\"model_name\"] = \"BramVanroy/Llama-2-13b-chat-dutch\"\n",
    "    #my_config.update({'model_name': st.session_state['model_name']})\n",
    "    llm_chatbot, loading_time = generate_with_llama_dutch(model_name)\n",
    "    st.session_state[\"model\"] = llm_chatbot\n",
    "    if loading_time < 60:\n",
    "        st.write(f\"Loading time: {loading_time:.1f} sec.\")\n",
    "    else:\n",
    "        st.write(f\"Loading time: {loading_time/60:.1f} min.\")\n",
    "    \n",
    "    \n",
    "# Text area to input text\n",
    "text = st.text_area(\"Enter text to summarize here.\")\n",
    "pprompt = st.text_area(\"Write your question/prompt here.\")\n",
    "\n",
    "if text and pprompt:\n",
    "    \n",
    "    # Display the model and input text\n",
    "    new_prompt = prompt.format(PersonalPrompt=pprompt,BODY=text)\n",
    "    print(f\"The follwoing prompt is used: {new_prompt}\")\n",
    "\n",
    "    st.write(\"Generating answer...\")\n",
    "    \n",
    "    out = get_answer(st.session_state[\"model\"], new_prompt)\n",
    "    st.write(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.3.1.3. Demo with predefined promt (More for Customer exposure) (code generator App.py)  \n",
    "- Input text field in the app, contains only the text that will be summarized, based on the predefined internal prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing App.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile App.py\n",
    "# This is just a very small test app to see if streamlit in runpod is actually working or not.\n",
    "import re\n",
    "import time\n",
    "import streamlit as st\n",
    "from transformers import pipeline, Conversation, AutoTokenizer\n",
    "\n",
    "#my_config = {'do_sample': True, 'temperature': 0.1, \n",
    "#             'repetition_penalty': 1.1, 'max_new_tokens': 500}\n",
    "#\"BramVanroy/Llama-2-13b-chat-dutch\"\n",
    "prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  \n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.  \n",
    "Please ensure that your responses are socially unbiased and positive in nature.\n",
    "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.  \n",
    "If you don't know the answer to a question, please don't share false information.\n",
    "«/SYS» \n",
    "Write a summary  in a couple of sentences for the following article. \n",
    "Article:  \"{BODY}\" [/INST]\"\"\"\n",
    "\n",
    "model_name = \"BramVanroy/Llama-2-13b-chat-dutch\"\n",
    "def generate_with_llama_dutch(model_name):\n",
    "    print(\"Into generate_with_llama_dutch\")\n",
    "    time_load_model_start = time.time()\n",
    "    \n",
    "    # Load the model and tokenizer outside of the functions\n",
    "    llm = pipeline(\"conversational\", \n",
    "                   model=model_name, \n",
    "                   tokenizer=AutoTokenizer.from_pretrained(model_name),\n",
    "                   do_sample=True, \n",
    "                   temperature=0.1, \n",
    "                   repetition_penalty=1.3,\n",
    "                   max_new_tokens=512,\n",
    "                   model_kwargs={\"device_map\": \"auto\",\"load_in_8bit\": True}\n",
    "                  )\n",
    "    time_load_model_end = time.time()\n",
    "    loading_time = time_load_model_end - time_load_model_start\n",
    "    print(f\"Elapsed time to load the model: {loading_time:.2f} sec\")\n",
    "    return llm, loading_time  \n",
    "\n",
    "def get_answer(chatbot, input_text):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        print(f\"Processing the input\\n {input_text}\\n\")\n",
    "        print('Processing the answer....')\n",
    "        \n",
    "        conversation = Conversation(input_text)\n",
    "        print(f\"Conversation is: {conversation}\\n\")\n",
    "        output = (chatbot(conversation))[1]['content']\n",
    "        print(f\"Output is:\\n==========\\n {output}\\n\")\n",
    "        # Calculate elapsed time and add it to the output\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        output += f\"\\n  ---> Answered in {elapsed_time:.1f} seconds, Nr generated words: {count_words(output)}\"\n",
    "        \n",
    "        return output\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error processing input: {e}\")\n",
    "        return None\n",
    "        \n",
    "    \n",
    "def count_words(text):\n",
    "    # Use a simple regular expression to count words\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return len(words)\n",
    "\n",
    "if \"model\" not in st.session_state.keys():\n",
    "    st.write(f\"Loading {model_name}..., Please wait.\" )\n",
    "    # Initialize the model with the default option\n",
    "    #st.session_state[\"model_name\"] = \"BramVanroy/Llama-2-13b-chat-dutch\"\n",
    "    #my_config.update({'model_name': st.session_state['model_name']})\n",
    "    llm_chatbot, loading_time = generate_with_llama_dutch(model_name)\n",
    "    st.session_state[\"model\"] = llm_chatbot\n",
    "    if loading_time < 60:\n",
    "        st.write(f\"Loading time: {loading_time:.1f} sec.\")\n",
    "    else:\n",
    "        st.write(f\"Loading time: {loading_time/60:.1f} min.\")\n",
    "    \n",
    "    \n",
    "# Text area to input text\n",
    "text = st.text_area(\"Enter text to summarize here.\")\n",
    "\n",
    "if text:\n",
    "    \n",
    "    # Display the model and input text\n",
    "    new_prompt = prompt.format(BODY=text)\n",
    "    print(f\"The follwoing prompt is used: {new_prompt}\")\n",
    "\n",
    "    st.write(\"Generating answer...\")\n",
    "    \n",
    "    out = get_answer(st.session_state[\"model\"], new_prompt)\n",
    "    st.write(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5.3.2. Launchin Demo: Streamlit   \n",
    "> 5.3.2.1. Using __tunneling__   (see prerequisites under 5.1 and 5.2)  \n",
    "> 5.3.2.2. Using __ngrok__ (see prerequisites under 5.2) alternative to tunneling  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Using tunneling (see prerequisites under 5.1 and 5.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 46.227.68.162     IP given, is passwd to load the url\n",
    "!streamlit run App.py & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.3.2.2 Using __ngrok__ (see prerequisites under 5.2) alternative to tunneling  \n",
    "- You need to create an account at ngrok\n",
    "- You need to setup your token (mine is 2cgHBiH2jr74RQpPsNV4b0mlGeL_7RmavDxxPepKATjTuJfpJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.1.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
      "Downloading pyngrok-7.1.2-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: pyngrok\n",
      "Successfully installed pyngrok-7.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n",
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
      "\n",
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Network URL: http://172.23.0.2:8501\n",
      "  External URL: http://87.197.140.238:8501\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "!ngrok authtoken 2cgHBiH2jr74RQpPsNV4b0mlGeL_7RmavDxxPepKATjTuJfpJ\n",
    "get_ipython().system_raw('nohup streamlit run App.py &')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NgrokTunnel: \"https://cee4-87-197-140-238.ngrok-free.app\" -> \"http://localhost:8501\">"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = ngrok.connect(8501)\n",
    "url #generates our URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# click on the first https link from <NgrokTunnel: \"https://cee4-87-197-140-238.ngrok-free.app\" -> \"http://localhost:8501\">\n",
    "!streamlit run --server.port 80 App.py > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3. __LlamaDutchDemoApp.py__ with LangChain  \n",
    "5.3.3.1. How to handle text to pdf:  \n",
    "5.3.3.2. How to do LangChain in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.3.3.1. How to handle text to pdf:\n",
    "Since LangChain was only working through pdf inputs, I am giving here the code how to add the input text from the streamlit app into a pdf file. This pdf file gets picked up by the app to generate the answer. The code below just test the handling of the input text to pdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile App.py\n",
    "import os\n",
    "import time\n",
    "import streamlit as st\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "# Text area to input text\n",
    "text = st.text_area(\"Enter text to summarize here.\")\n",
    "local_pdf_name = 'local.pdf'\n",
    "\n",
    "def create_pdf(list_strings, filename):\n",
    "    # Create a canvas with letter size (8.5x11 inches)\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "\n",
    "    # Set font and font size for first page\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    \n",
    "    for string in list_strings:\n",
    "        # Draw first string on the first page\n",
    "        draw_multiline_string(c, 100, 700, string)\n",
    "\n",
    "        # Add a new page\n",
    "        c.showPage()\n",
    "\n",
    "    # Save the PDF\n",
    "    c.save()\n",
    "\n",
    "def draw_multiline_string(canvas, x, y, text, max_width=400, line_spacing=15):\n",
    "    lines = []\n",
    "    current_line = ''\n",
    "    for word in text.split():\n",
    "        if canvas.stringWidth(current_line + ' ' + word) <= max_width:\n",
    "            current_line += ' ' + word\n",
    "        else:\n",
    "            lines.append(current_line.strip())\n",
    "            current_line = word\n",
    "    lines.append(current_line.strip())\n",
    "\n",
    "    for line in lines:\n",
    "        canvas.drawString(x, y, line)\n",
    "        y -= line_spacing\n",
    "\n",
    "# processes each time a new text is entered in the input field\n",
    "if text:\n",
    "    print(text)\n",
    "    if os.path.exists(local_pdf_name):\n",
    "        os.remove(local_pdf_name)\n",
    "        \n",
    "    create_pdf([text], local_pdf_name)\n",
    "    loader = PyPDFLoader(local_pdf_name)\n",
    "    docs = loader.load()\n",
    "    print(docs)\n",
    "    # Display the model and input text\n",
    "    out = \"\\nProcessing, Please wait for the answer...\\n\"\n",
    "    st.write(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 5.3.3.2. How to do Lanchain in the notebook:\n",
    "Here follows code that runs without the streamlit app (this is a variation on the code from 4.8.4)  \n",
    "This was created to see if it worked and the next step was taking this code and wrapping the streamlit around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline, Conversation, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "#1: \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "#2: \"BramVanroy/Llama-2-13b-chat-dutch\"\n",
    "\n",
    "my_config = {'model_name': \"BramVanroy/Llama-2-13b-chat-dutch\", #\"./Bram\", #BramVanroy/Llama-2-13b-chat-dutch\", \n",
    "             'do_sample': True, 'temperature': 0.1, \n",
    "             'repetition_penalty': 1.1, 'max_new_tokens': 500, }\n",
    "\n",
    "print(f\"Selected model: {my_config['model_name']}\")\n",
    "print(f\"Parameters are: {my_config}\")\n",
    "local_pdf_name = 'local.pdf'\n",
    "\n",
    "def generate_with_llama_chat(my_config):\n",
    "    start_time = time.time()\n",
    "    print('tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(my_config['model_name'])\n",
    "    print('causal')\n",
    "    model = AutoModelForCausalLM.from_pretrained(my_config['model_name'])\n",
    "    print('Pipeline')\n",
    "    chatbot = pipeline(\"text-generation\",model=my_config['model_name'], \n",
    "                       tokenizer=tokenizer,\n",
    "                       do_sample=my_config['do_sample'], \n",
    "                       temperature=my_config['temperature'], \n",
    "                       repetition_penalty=my_config['repetition_penalty'],\n",
    "                       #max_length=my_config['max_length'],\n",
    "                       max_new_tokens=my_config['max_new_tokens'], \n",
    "                       model_kwargs={\"device_map\": \"auto\",\"load_in_8bit\": True})\n",
    "    llm = HuggingFacePipeline(pipeline=chatbot)\n",
    "    loading_time = time.time()-start_time\n",
    "    return llm, loading_time\n",
    "\n",
    "def create_pdf(list_strings, filename):\n",
    "    # Create a canvas with letter size (8.5x11 inches)\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "\n",
    "    # Set font and font size for first page\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    \n",
    "    for string in list_strings:\n",
    "        # Draw first string on the first page\n",
    "        draw_multiline_string(c, 100, 700, string)\n",
    "\n",
    "        # Add a new page\n",
    "        c.showPage()\n",
    "\n",
    "    # Save the PDF\n",
    "    c.save()\n",
    "\n",
    "def draw_multiline_string(canvas, x, y, text, max_width=400, line_spacing=15):\n",
    "    lines = []\n",
    "    current_line = ''\n",
    "    for word in text.split():\n",
    "        if canvas.stringWidth(current_line + ' ' + word) <= max_width:\n",
    "            current_line += ' ' + word\n",
    "        else:\n",
    "            lines.append(current_line.strip())\n",
    "            current_line = word\n",
    "    lines.append(current_line.strip())\n",
    "\n",
    "    for line in lines:\n",
    "        canvas.drawString(x, y, line)\n",
    "        y -= line_spacing    \n",
    "        \n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "def langchain_block(llm, map_template, reduce_template):\n",
    "    map_prompt = PromptTemplate.from_template(map_template)\n",
    "    map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "    \n",
    "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "    reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "    \n",
    "    combine_documents_chain = StuffDocumentsChain(\n",
    "        llm_chain=reduce_chain, document_variable_name=\"doc_maps\"\n",
    "    )\n",
    "\n",
    "    # Combines and iteratively reduces the mapped documents\n",
    "    reduce_documents_chain = ReduceDocumentsChain(\n",
    "        # This is final chain that is called.\n",
    "        combine_documents_chain=combine_documents_chain,\n",
    "        # If documents exceed context for `StuffDocumentsChain`\n",
    "        collapse_documents_chain=combine_documents_chain,\n",
    "        # The maximum number of tokens to group documents into.\n",
    "        token_max=4096\n",
    "    )\n",
    "\n",
    "    map_reduce_chain = MapReduceDocumentsChain(\n",
    "        # Map chain\n",
    "        llm_chain=map_chain,\n",
    "        # Reduce chain\n",
    "        reduce_documents_chain=reduce_documents_chain,\n",
    "        # The variable name in the llm_chain to put the documents in\n",
    "        document_variable_name=\"docs\",\n",
    "        # Return the results of the map steps in the output\n",
    "        return_intermediate_steps=True\n",
    "    )\n",
    "\n",
    "    text_splitter = TokenTextSplitter(chunk_size=800, chunk_overlap=0)    \n",
    "    return map_reduce_chain, text_splitter\n",
    "    \n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Map    \n",
    "map_template = \"\"\"The following is a set of documents\n",
    "    {docs}\n",
    "    Based on this list of docs, please make summaries. Do not summarize the document when there are no full sentences in the document. \n",
    "    Helpful Answer:\"\"\"\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "    {doc_maps}\n",
    "    Take these and distill it into a final, consolidated summary of the main themes. \n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "loader = PyPDFLoader('history1_be_gr_nl.pdf')\n",
    "docs = loader.load()\n",
    "\n",
    "llm_chatbot, loading_time = generate_with_llama_chat(my_config)\n",
    "map_reduce_chain, text_splitter = langchain_block(llm_chatbot, map_template, reduce_template)\n",
    "\n",
    "# For pdf documents you can use the following documents splitter\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"Number of splits= {len(split_docs)}\")\n",
    "\n",
    "map_reduce_output1 = map_reduce_chain.invoke(split_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the create_pd_from_map_reduce_output from section 4.4\n",
    "pdf_mp_summary = create_pd_from_map_reduce_output(map_reduce_output1)\n",
    "\n",
    "# code that prints the output in a human readable format \n",
    "for i in range(len(pdf_mp_summary)):\n",
    "   print(f\"\\n ========================================\\n{pdf_mp_summary.iloc[i]['chunks']} \\n+++++++++++++++++++++++\\n{pdf_mp_summary.iloc[i]['concise_summary']}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_output1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3.3. How to do Lanchain in the streamlit app:\n",
    "It will take the text from the streamlit input field, create a pdf file of 1 page and then use that pdf to generate the answer.  \n",
    "There is probably a faster way without going over a pdf but i did not further investigate this since i had a workin example using a pdf file.\n",
    "\n",
    "- If you want to test this with 'history1_be_gr_nl.pdf', you need to generate it first using the cells from section 3.2.3.2. and you need to uncomment the line  \n",
    "``` #loader = PyPDFLoader('history1_be_gr_nl.pdf')```   \n",
    "from the code bellow (around line 168)\n",
    "\n",
    "```python\n",
    "    # loading this generated pdf (is called local_pdf_name = 'local.pdf')\n",
    "    loader = PyPDFLoader(local_pdf_name)\n",
    "    \n",
    "    # if you want to load an example with 3 pages you can use the following line\n",
    "    #loader = PyPDFLoader('history1_be_gr_nl.pdf')\n",
    "```  \n",
    "- There is /was a warning that needs to be taken care of:\n",
    "```\n",
    "Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
    "\n",
    "`from langchain_community.llms import HuggingFacePipeline`.\n",
    "\n",
    "To install langchain-community run `pip install -U langchain-community`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting App.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile App.py\n",
    "import time\n",
    "import streamlit as st\n",
    "from pathlib import Path as p\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import pipeline, Conversation, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "#1: \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "#2: \"BramVanroy/Llama-2-13b-chat-dutch\"\n",
    "\n",
    "my_config = {'model_name': \"BramVanroy/Llama-2-13b-chat-dutch\", #\"./Bram\", #BramVanroy/Llama-2-13b-chat-dutch\", \n",
    "             'do_sample': True, 'temperature': 0.1, \n",
    "             'repetition_penalty': 1.1, 'max_new_tokens': 500, }\n",
    "\n",
    "print(f\"Selected model: {my_config['model_name']}\")\n",
    "print(f\"Parameters are: {my_config}\")\n",
    "local_pdf_name = 'local.pdf'\n",
    "\n",
    "def generate_with_llama_chat(my_config):\n",
    "    start_time = time.time()\n",
    "    print('tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(my_config['model_name'])\n",
    "    print('causal')\n",
    "    model = AutoModelForCausalLM.from_pretrained(my_config['model_name'])\n",
    "    print('Pipeline')\n",
    "    chatbot = pipeline(\"text-generation\",model=my_config['model_name'], \n",
    "                       tokenizer=tokenizer,\n",
    "                       do_sample=my_config['do_sample'], \n",
    "                       temperature=my_config['temperature'], \n",
    "                       repetition_penalty=my_config['repetition_penalty'],\n",
    "                       #max_length=my_config['max_length'],\n",
    "                       max_new_tokens=my_config['max_new_tokens'], \n",
    "                       model_kwargs={\"device_map\": \"auto\",\"load_in_8bit\": True})\n",
    "    llm = HuggingFacePipeline(pipeline=chatbot)\n",
    "    loading_time = time.time()-start_time\n",
    "    return llm, loading_time\n",
    "\n",
    "def create_pdf(list_strings, filename):\n",
    "    # Create a canvas with letter size (8.5x11 inches)\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "\n",
    "    # Set font and font size for first page\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    \n",
    "    for string in list_strings:\n",
    "        # Draw first string on the first page\n",
    "        draw_multiline_string(c, 100, 700, string)\n",
    "\n",
    "        # Add a new page\n",
    "        c.showPage()\n",
    "\n",
    "    # Save the PDF\n",
    "    c.save()\n",
    "\n",
    "def draw_multiline_string(canvas, x, y, text, max_width=400, line_spacing=15):\n",
    "    lines = []\n",
    "    current_line = ''\n",
    "    for word in text.split():\n",
    "        if canvas.stringWidth(current_line + ' ' + word) <= max_width:\n",
    "            current_line += ' ' + word\n",
    "        else:\n",
    "            lines.append(current_line.strip())\n",
    "            current_line = word\n",
    "    lines.append(current_line.strip())\n",
    "\n",
    "    for line in lines:\n",
    "        canvas.drawString(x, y, line)\n",
    "        y -= line_spacing    \n",
    "        \n",
    "\n",
    "if \"model\" not in st.session_state.keys():\n",
    "    st.write(f\"Loading {my_config['model_name']}..., Please wait.\" )\n",
    "    # Initialize the model with the default option\n",
    "    #st.session_state[\"model_name\"] = \"BramVanroy/Llama-2-13b-chat-dutch\"\n",
    "    #my_config.update({'model_name': st.session_state['model_name']})\n",
    "    llm_chatbot, loading_time = generate_with_llama_chat(my_config)\n",
    "    st.session_state[\"model\"] = llm_chatbot\n",
    "    st.write(f\"Loading time: {loading_time/60:.1f} min.\")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Map    \n",
    "map_template = \"\"\"The following is a set of documents\n",
    "    {docs}\n",
    "    Based on this list of docs, please make summaries. Do not summarize the document when there are no full sentences in the document. \n",
    "    Helpful Answer:\"\"\"\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "    {doc_maps}\n",
    "    Take these and distill it into a final, consolidated summary of the main themes. \n",
    "    Helpful Answer:\"\"\"\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "def langchain_block(llm, map_template, reduce_template):\n",
    "    map_prompt = PromptTemplate.from_template(map_template)\n",
    "    map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "    \n",
    "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "    reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "    \n",
    "    combine_documents_chain = StuffDocumentsChain(\n",
    "        llm_chain=reduce_chain, document_variable_name=\"doc_maps\"\n",
    "    )\n",
    "\n",
    "    # Combines and iteratively reduces the mapped documents\n",
    "    reduce_documents_chain = ReduceDocumentsChain(\n",
    "        # This is final chain that is called.\n",
    "        combine_documents_chain=combine_documents_chain,\n",
    "        # If documents exceed context for `StuffDocumentsChain`\n",
    "        collapse_documents_chain=combine_documents_chain,\n",
    "        # The maximum number of tokens to group documents into.\n",
    "        token_max=4096\n",
    "    )\n",
    "\n",
    "    map_reduce_chain = MapReduceDocumentsChain(\n",
    "        # Map chain\n",
    "        llm_chain=map_chain,\n",
    "        # Reduce chain\n",
    "        reduce_documents_chain=reduce_documents_chain,\n",
    "        # The variable name in the llm_chain to put the documents in\n",
    "        document_variable_name=\"docs\",\n",
    "        # Return the results of the map steps in the output\n",
    "        return_intermediate_steps=True\n",
    "    )\n",
    "\n",
    "    text_splitter = TokenTextSplitter(chunk_size=800, chunk_overlap=0)    \n",
    "    return map_reduce_chain, text_splitter\n",
    "\n",
    "def create_pd_from_map_reduce_output(map_reduce_outputs):\n",
    "    data_folder = p.cwd() \n",
    "    p(data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    final_mp_data = []\n",
    "    for doc, out in zip(map_reduce_outputs[\"input_documents\"], map_reduce_outputs[\"intermediate_steps\"]):\n",
    "        output = {}\n",
    "        output[\"file_name\"] = p(doc.metadata[\"source\"]).stem\n",
    "        output[\"file_type\"] = p(doc.metadata[\"source\"]).suffix\n",
    "        output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "        output[\"chunks\"] = doc.page_content\n",
    "        output[\"concise_summary\"] = out\n",
    "        final_mp_data.append(output)\n",
    "\n",
    "    pdf_mp_summary = pd.DataFrame.from_dict(final_mp_data)\n",
    "    pdf_mp_summary = pdf_mp_summary.sort_values(by=[\"file_name\", \"page_number\"])  # sorting the dataframe by filename and page_number\n",
    "    pdf_mp_summary.reset_index(inplace=True, drop=True)\n",
    "    pdf_mp_summary\n",
    "    return pdf_mp_summary\n",
    "\n",
    "# Text area to input text\n",
    "text = st.text_area(\"Enter text to summarize here.\")\n",
    "\n",
    "# processes each time a new text is entered in the input field\n",
    "if text:\n",
    "    # creates a pdf file from the input text from the streamlit app \n",
    "    create_pdf([text], local_pdf_name)\n",
    "\n",
    "    # loading this generated pdf (is called local_pdf_name = 'local.pdf')\n",
    "    loader = PyPDFLoader(local_pdf_name)\n",
    "    \n",
    "    # if you want to load an example with 3 pages you can use the following line\n",
    "    #loader = PyPDFLoader('history1_be_gr_nl.pdf')\n",
    "\n",
    "    docs = loader.load()\n",
    "    map_reduce_chain, text_splitter = langchain_block(st.session_state[\"model\"], map_template, reduce_template)\n",
    "    # For pdf documents you can use the following documents splitter\n",
    "    split_docs = text_splitter.split_documents(docs)\n",
    "    print(f\"Number of splits= {len(split_docs)}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    out = \"\\nProcessing, Please wait for the answer...\\n\"\n",
    "    st.write(out)\n",
    "    \n",
    "    map_reduce_output1 = map_reduce_chain.invoke(split_docs)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(map_reduce_output1)\n",
    "    print(f\"\\n\\nElapsed time:  {elapsed_time}\")\n",
    "    pdf_mp_summary = create_pd_from_map_reduce_output(map_reduce_output1)\n",
    "\n",
    "    # code that prints the output in a human readable format \n",
    "    for i in range(len(pdf_mp_summary)):\n",
    "       print(f\"\\n ========================================\")\n",
    "       print(f\"{pdf_mp_summary.iloc[i]['chunks']}\")\n",
    "       print(f\"+++++++++++++++++++++++\")\n",
    "       print(f\"{pdf_mp_summary.iloc[i]['concise_summary']}\" )\n",
    "       st.write(f\"Part{i}: {pdf_mp_summary.iloc[i]['chunks']}\")\n",
    "       st.write(f\"{pdf_mp_summary.iloc[i]['concise_summary']}\")\n",
    "        \n",
    "    st.write(f\"\\n\\nElapsed time:  {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.23.0.2:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://87.197.140.238:8501\u001b[0m\n",
      "\u001b[0m\n",
      "your url is: https://ready-eagles-love.loca.lt\n",
      "/usr/local/lib/python3.10/dist-packages/langchain/llms/__init__.py:548: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.llms import HuggingFacePipeline`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/__init__.py:36: LangChainDeprecationWarning: Importing document loaders from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.document_loaders import PyPDFLoader`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "Selected model: BramVanroy/Llama-2-13b-chat-dutch\n",
      "Parameters are: {'model_name': 'BramVanroy/Llama-2-13b-chat-dutch', 'do_sample': True, 'temperature': 0.1, 'repetition_penalty': 1.1, 'max_new_tokens': 500}\n",
      "tokenizer\n",
      "causal\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:06<00:00,  2.01s/it]\n",
      "Pipeline\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:06<00:00,  2.18s/it]\n",
      "/usr/local/lib/python3.10/dist-packages/langchain/llms/__init__.py:548: LangChainDeprecationWarning: Importing LLMs from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.llms import HuggingFacePipeline`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/__init__.py:36: LangChainDeprecationWarning: Importing document loaders from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.document_loaders import PyPDFLoader`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "Selected model: BramVanroy/Llama-2-13b-chat-dutch\n",
      "Parameters are: {'model_name': 'BramVanroy/Llama-2-13b-chat-dutch', 'do_sample': True, 'temperature': 0.1, 'repetition_penalty': 1.1, 'max_new_tokens': 500}\n",
      "Number of splits= 5\n"
     ]
    }
   ],
   "source": [
    "# 104.255.9.187\n",
    "# 87.197.140.238\n",
    "!streamlit run App.py & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
