{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# General info & Useful documentation\n",
    "- When loading the model generates the following error, try to restart the kernel and retry:\n",
    "\n",
    "  Error: _Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n",
    "          the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n",
    "          these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n",
    "          `device_map` to `from_pretrained`. Check\n",
    "          https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n",
    "          for more details._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Useful documentation:\n",
    "- __LangChain__: [Stuff, Map-Reduce & Refine](https://python.langchain.com/docs/use_cases/summarization)\n",
    "- __LangChain__: [Quick start](https://python.langchain.com/docs/get_started/quickstart)\n",
    "- __LangChain__ __HuggingFace__: [Click here](https://python.langchain.com/docs/integrations/chat/huggingface)\n",
    "  1) Utilize the HuggingFaceTextGenInference, HuggingFaceEndpoint, or HuggingFaceHub integrations to instantiate an LLM.\n",
    "  2) Utilize the ChatHuggingFace class to enable any of these LLMs to interface with LangChain’s Chat Messages abstraction.\n",
    "  3) Demonstrate how to use an open-source LLM to power an ChatAgent pipeline\n",
    "- __AutoModelForCausalLM__: the way we managed to load the dutch model for LangChain processing is described [here](https://python.langchain.com/docs/integrations/llms/huggingface_pipelines)\n",
    "- __LlamaCpp__: [Parameters description](https://api.python.langchain.com/en/latest/llms/langchain_community.llms.llamacpp.LlamaCpp.html#)\n",
    "- __Google Cloud Generative AI__ - Language __GitHub__ repository: [Click here](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/use-cases/document-summarization/summarization_large_documents_langchain.ipynb)\n",
    "- __Medium__ article: [Generating Summaries for Large Documents with Llama2 using Hugging Face and Langchain](https://medium.com/@ankit941208/generating-summaries-for-large-documents-with-llama2-using-hugging-face-and-langchain-f7de567339d2)\n",
    "- [Pinecone LangChain AI Handbook](https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/#ConversationChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pip installs, Different Inputs (Pdf generation, Input texts, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFUPwRcLRBTO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n",
    "!pip install langchain\n",
    "!pip install huggingface_hub\n",
    "!pip install tiktoken\n",
    "!pip install pypdf\n",
    "!pip install reportlab\n",
    "!pip install pandas\n",
    "!pip install langdetect\n",
    "# download the gguf file for the Llama Dutch model, when Cpp method is used \n",
    "#!huggingface-cli download TheBloke/Llama-2-13B-GGUF llama-2-13b.Q6_K.gguf --local-dir . --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This was an attempt in order to be able to load the model for LangChain (in the end this is not been used)\n",
    "#!huggingface-cli download BramVanroy/Llama-2-13b-chat-dutch --local-dir ./Bram --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check if this cell is still needed or not:\n",
    "import time\n",
    "start_time = time.time()\n",
    "!pip install --upgrade transformers\n",
    "\n",
    "!pip install accelerate                 # Necessary for llama model\n",
    "!pip install bitsandbytes\n",
    "!pip install langdetect\n",
    "!pip install google-cloud-translate\n",
    "!pip install -U huggingface-hub\n",
    "\n",
    "# T5 needs the following package:\n",
    "try:\n",
    "    import sentencepiece\n",
    "except ImportError:\n",
    "    print(\"SentencePiece is not installed. Installing...\")\n",
    "    !pip install sentencepiece\n",
    "    print(\"SentencePiece installed successfully.\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Pip install: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Different Inputs:\n",
    "Sections are:\n",
    "1) text\n",
    "2) pdf\n",
    "3) text transformer to pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_text = '''Landelijk dekkend netwerk van infrastructuren\n",
    "Origineel: Tweede KamerDatum: 21-01-2024\n",
    "Tweede Kamer der Staten-Generaal\n",
    "2\n",
    "Vergaderjaar 2023–2024\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "27 529 Informatie- en Communicatietechnologie (ICT) in\n",
    "de Zorg\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Nr. 313 BRIEF VAN DE MINISTER VAN VOLKSGEZONDHEID, WELZIJN EN\n",
    "SPORT\n",
    "\n",
    "Aan de Voorzitter van de Tweede Kamer der Staten-Generaal\n",
    "\n",
    "Den Haag, 22 januari 2024\n",
    "\n",
    "De gezondheidszorg staat onder grote druk; door de vergrijzing groeit de\n",
    "vraag naar zorg, terwijl er minder capaciteit beschikbaar is door een\n",
    "toenemend personeelstekort. De regeldruk en administratieve lasten zijn\n",
    "hoog en zorgverleners zijn veel tijd kwijt aan het verzamelen, overnemen\n",
    "en invoeren van de benodigde informatie voor het verlenen van goede\n",
    "zorg. De zorgverlening vindt in toenemende mate plaats in een netwerk\n",
    "van meerdere partijen, wat beter ondersteund moet worden met snelle en\n",
    "veilige gegevensuitwisseling. Tegelijkertijd groeit de behoefte aan data\n",
    "voor secundair gebruik ten behoeve van wetenschappelijk onderzoek,\n",
    "kwaliteitsmonitoring, gezondheidsbeleid en AI-toepassingen. De noodzaak\n",
    "om een landelijk dekkend netwerk van infrastructuren – voor de volle\n",
    "breedte van de zorg – te realiseren, voor het uitwisselen en beschikbaar\n",
    "stellen van relevante data aan patiënt, zorgverlener en onderzoeker,\n",
    "neemt toe. Daarom heeft het zorgveld aan het Ministerie van VWS\n",
    "gevraagd regie te pakken en sturing te geven aan de realisatie van een\n",
    "landelijk dekkend netwerk.\n",
    "\n",
    "In de brief van mijn ambtsvoorganger van 13 april 2023 over het Landelijk\n",
    "dekkend netwerk van infrastructuren1 heeft hij uw Kamer geïnformeerd\n",
    "over de analyse, die op zijn verzoek is uitgevoerd, naar scenario's die\n",
    "aanvullend op het huidige beleid een toekomstbestendig landelijk\n",
    "dekkend netwerk realiseren. Hieruit kwam het advies om gelijktijdig een\n",
    "dubbele beweging te omarmen en te stimuleren:\n",
    "Een gedistribueerd communicatienetwerk voor (geprotocolleerde)\n",
    "overdracht van gegevens door één-op-één communicatie tussen\n",
    "zorgverleners én om data beschikbaar te kunnen stellen voor andere\n",
    "doeleinden.\n",
    "\n",
    "\n",
    "\n",
    "1\n",
    "Kamerstukken II 2022/23, 27 529, nr. 293\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "kst-27529-313\n",
    "ISSN 0921 - 7371\n",
    "'s-Gravenhage 2024 Tweede Kamer, vergaderjaar 2023–2024, 27 529, nr. 313 1\n",
    "Een data-centrische oplossing, bestaande uit gekoppelde dataplatfor-\n",
    "men, die het gebruik van data scheidt van de functionaliteit en\n",
    "databeschikbaarheid voor primair en secundair gebruik, alsmede\n",
    "gezamenlijke dossiervorming in de context van netwerkzorg, faciliteert.\n",
    "Mijn ambtsvoorganger heeft daarbij aangegeven beide geadviseerde\n",
    "richtingen, met de kennis en expertise van veldpartijen, verder uit te laten\n",
    "werken. In deze brief geef ik invulling aan de toezegging om uw Kamer te\n",
    "informeren over de uitwerking van de geadviseerde richting en de te\n",
    "nemen vervolgstappen om tot een landelijk dekkend netwerk te komen.\n",
    "\n",
    "De infrastructuur moet de uitdagingen van de zorg ondersteunen\n",
    "\n",
    "De technologische ontwikkelingen gaan razendsnel en bieden veel\n",
    "potentie voor het verbeteren van de kwaliteit van de zorg en het vermin-\n",
    "deren van de administratieve lasten. Er worden regionaal allerlei\n",
    "initiatieven gestart om toepassingen te ontwikkelen, die netwerk- en\n",
    "hybride zorg kunnen ondersteunen. Belangrijke innovaties, want met de\n",
    "juiste en volledige data kan een zorgverlener betere en veilige zorg\n",
    "leveren. En kunnen burgers meebeslissen over voor hen passende zorg.\n",
    "We moeten echter voorkomen dat de huidige versnippering in het\n",
    "zorglandschap continueert en zelfs versterkt wordt, door allerlei nieuwe\n",
    "initiatieven met eigen standaarden die niet onderling interoperabel zijn. Er\n",
    "is behoefte aan een solide en toekomstbestendige landelijke infra-\n",
    "structuur, waarmee het zorgproces ondersteund wordt en waarop verdere\n",
    "technologische ontwikkelingen kunnen plaats vinden. Open internationale\n",
    "standaarden moeten hierbij het uitgangspunt vormen, opdat er geprofi-\n",
    "teerd kan worden van internationale kennis en innovaties en de data\n",
    "uiteindelijk niet alleen landelijk maar ook Europees via de European\n",
    "Health Data Space (EHDS) kan stromen.\n",
    "\n",
    "In de vorige brief is benoemd dat zorginformatie vaak binnen één bepaald\n",
    "zorgproces wordt vastgelegd en gedeeld; er vindt een geprotocolleerde\n",
    "overdracht van gegevens plaats. Denk hierbij aan een verwijzing van een\n",
    "huisarts naar een specialist, of de overdracht van ziekenhuis naar\n",
    "verpleeghuis of wijkverpleging. De geprioriteerde gegevensuitwisselingen\n",
    "onder de Wet elektronische gegevensuitwisseling in de zorg (Wegiz)\n",
    "ondersteunen dit. In de Nationale visie en strategie op het gezondheidsin-\n",
    "formatiestelsel2 (NVS) wordt het belang van deze geprotocolleerde\n",
    "overdrachten onderschreven, maar wordt ook onderkend dat alleen het\n",
    "realiseren van een infrastructuur voor geprotocolleerde gegevensuit-\n",
    "wisseling niet voldoende is om zorgverleners te faciliteren bij het verlenen\n",
    "van hybride en netwerkzorg. Daarom willen we van gegevensuitwisseling\n",
    "doorgroeien naar databeschikbaarheid.\n",
    "\n",
    "Een communicatienetwerk voor gegevensuitwisseling\n",
    "\n",
    "Voor gegevensuitwisseling is een communicatienetwerk nodig. In het\n",
    "geadviseerde scenario (een gedistribueerd communicatienetwerk) heeft\n",
    "elke zorgaanbieder zijn eigen knooppunt en kunnen zorgaanbieders\n",
    "onderling tussen deze knooppunten gegevens uitwisselen zonder\n",
    "tussenkomst van een derde partij.\n",
    "\n",
    "De verdere uitwerking van dit scenario en de afstemming hierover met\n",
    "veldpartijen heeft tot de conclusie geleid dat dit scenario momenteel een\n",
    "nog te grote stap is. Aangezien veel zorgaanbieders al zijn aangesloten op\n",
    "een zorginfrastructuur (bijvoorbeeld LSP en Chipsoft Zorgplatform) is het\n",
    "efficiënter om eerst toe te werken naar een hybride situatie. In deze\n",
    "hybride situatie ontsluiten bestaande infrastructuren de data bij hun\n",
    "\n",
    "2\n",
    "Kamerstukken II, 2022/23, 27 529, nr. 292\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tweede Kamer, vergaderjaar 2023–2024, 27 529, nr. 313 2\n",
    "gekoppelde zorgaanbieders en stellen die gegevens vervolgens\n",
    "beschikbaar via een (gezamenlijk) knooppunt. Een zorgaanbieder zonder\n",
    "zorginfrastructuur kan een eigen knooppunt gebruiken. De verzameling\n",
    "van al deze knooppunten vormt het communicatienetwerk.\n",
    "Dit biedt alle zorgaanbieders de mogelijkheid om onderling (rechtstreeks\n",
    "via een eigen knooppunt of via een bestaande infrastructuur) gegevens uit\n",
    "te wisselen.\n",
    "Om deze knooppunten te ontwikkelen en verbinden is het noodzakelijk om\n",
    "te komen tot een landelijke kaderstelling voor standaardisatie van taal en\n",
    "techniek, in de vorm van een landelijk vertrouwensstelsel (LVS)3. Het LVS\n",
    "omvat het geheel aan technische, organisatorische en juridische\n",
    "afspraken die zorgt voor vertrouwen in de landelijke elektronische\n",
    "gegevensuitwisseling en het gebruik van gezondheidsgegevens.\n",
    "Generieke functies en bijbehorende voorzieningen vormen een belangrijk\n",
    "onderdeel van dit stelsel. In de recente Kamerbrief over Generieke\n",
    "functies4 benoemt mijn ambtsvoorganger de interventies die nodig zijn\n",
    "om de randvoorwaardelijke generieke functies en voorzieningen te\n",
    "realiseren.\n",
    "\n",
    "Doorgroeien naar databeschikbaarheid\n",
    "\n",
    "We willen dat patiënten, zorgverleners en onderzoekers digitaal kunnen\n",
    "beschikken over de juiste informatie, op het juiste moment en op de juiste\n",
    "plek. Hiervoor zijn toepassingen nodig die data beschikbaar stellen voor\n",
    "preventie, het primaire zorgproces en secundair datagebruik5. Deze\n",
    "toepassingen maken gebruik van het communicatienetwerk voor\n",
    "gegevensuitwisseling om bij meerdere betrokken zorgaanbieders\n",
    "(tegelijkertijd) actuele en relevante zorggegevens op te vragen.\n",
    "\n",
    "Veel initiatieven voor dergelijke toepassingen stagneren omdat het lastig\n",
    "is om op een eenduidige wijze bij verschillende zorgaanbieders data te\n",
    "ontsluiten en samen te voegen. Dit komt doordat de verschillende\n",
    "infrastructuren en knooppunten nog niet met elkaar verbonden zijn, maar\n",
    "ook doordat zorgaanbieders hun data nog niet (voldoende) gestandaardi-\n",
    "seerd vastleggen.\n",
    "\n",
    "Er wordt vaak gesproken over data uit de bron, maar «de bron» is niet\n",
    "altijd geschikt voor (gestandaardiseerde) dataopslag en -ontsluiting. Het\n",
    "Elektronisch Patiënten Dossier (EPD) en Elektronisch Cliënt Dossier (ECD)\n",
    "zijn systemen voor zorgaanbieders, die initieel zijn toegespitst op het\n",
    "functioneel gebruik binnen de organisatie. Om tijdig de juiste data voor\n",
    "gegevensdeling beschikbaar te stellen, moet een EPD/ECD systeem\n",
    "functionaliteit en data goed van elkaar kunnen scheiden. Tevens is er niet\n",
    "altijd sprake van één bronsysteem; in sommige sectoren werken\n",
    "zorgaanbieders met meerdere applicaties. Met name in de tweede-\n",
    "lijnszorg kiezen steeds meer zorgaanbieders ervoor om data uit één of\n",
    "meerdere bronsystemen separaat op te slaan (datawarehouse of data\n",
    "lake). De data wordt in principe onbewerkt en ongestructureerd\n",
    "gekopieerd vanuit de bron. De vertaling van die data naar een gestandaar-\n",
    "diseerd datamodel kan in dezelfde opslagomgeving plaatsvinden – men\n",
    "spreekt dan over een dataplatform – of wordt als separate functionaliteit\n",
    "los van de opslag uitgevoerd. Wanneer een EPD- of ECD-systeem niet de\n",
    "data volgens het afgesproken informatiemodel op kan slaan en/of als er\n",
    "sprake is van meerdere bronsystemen, kan een dataplatform een\n",
    "\n",
    "3\n",
    "Eén van de randvoorwaarden voor gegevensuitwisseling zoals beschreven in de brief van\n",
    "13 april 2023\n",
    "4\n",
    "Kamerstukken II, 2022/23, 27 529, nr. 312\n",
    "5\n",
    "Mits daarvoor toestemming is gegeven door de patiënt en aan alle voorwaarden ten aanzien\n",
    "van identificatie, authenticatie en autorisatie is voldaan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tweede Kamer, vergaderjaar 2023–2024, 27 529, nr. 313 3\n",
    "gewenste aanvulling zijn voor een zorgaanbieder. Daarbij is het\n",
    "uitgangspunt dat de data onder de verantwoordelijkheid en invloedssfeer\n",
    "van de zorgaanbieder blijft.\n",
    "\n",
    "Inspringen op een behoefte: netwerk- en integratiediensten\n",
    "\n",
    "Na het verder uitwerken van de twee geadviseerde scenario's, vanuit het\n",
    "architectuur- en techniekperspectief, zijn de contouren getoetst in de\n",
    "praktijk. Aanbieders van regionale en sectorale toepassingen, die een\n",
    "bijdrage leveren aan het realiseren van databeschikbaarheid, hebben de\n",
    "functionaliteit van hun oplossing schriftelijk beschreven en vervolgens\n",
    "persoonlijk toegelicht. Deze gesprekken en documentatie hebben\n",
    "inzichtelijk gemaakt waar de uitdagingen en knelpunten zitten.\n",
    "\n",
    "De standaardisatie van taal en techniek gaat niet snel genoeg om de\n",
    "toenemende vraag naar toepassingen voor databeschikbaarheid te\n",
    "faciliteren. Aanbieders van toepassingen moeten zorgaanbieders (en\n",
    "diens leveranciers) ondersteunen bij het ontsluiten van de data en\n",
    "daarnaast nog allerlei netwerk- en integratiediensten (bijvoorbeeld het\n",
    "verzamelen, vertalen, valideren, samenvoegen en verrijken van data)\n",
    "aanbieden om de data bruikbaar te maken voor de toepassingen. Nu\n",
    "worden al deze «diensten» voor elke toepassing separaat en opnieuw\n",
    "ontwikkeld en uitgevoerd; hiermee gaat veel tijd en geld verloren. Ook\n",
    "moet voorkomen worden dat databeschikbaarheid een verdienmodel\n",
    "wordt; het hele proces van data lokaliseren, opvragen, beschikbaar\n",
    "stellen, verzamelen en integreren moet non-concurrentieel zijn. Toepas-\n",
    "singen en diensten daarentegen mogen – met name voor de meer\n",
    "specifieke functionaliteit – wel concurrentieel zijn om ontwikkeling en\n",
    "innovatie te bevorderen.\n",
    "\n",
    "Inmiddels hebben een aantal partijen en zorgkoepels6 hun ambities en\n",
    "krachten gebundeld in de CumuluZ-coalitie met als doel toe te werken\n",
    "naar een landelijke data infrastructuur met één non-concurrentieel\n",
    "data-integratieplatform, die de data bij zorgaanbieders ontsluit, verwerkt\n",
    "en vervolgens beschikbaar stelt aan toepassingen en diensten.\n",
    "\n",
    "Aanscherping van de voorgestelde scenario's\n",
    "\n",
    "Zoals in het begin van deze brief benoemd zijn de volgende twee\n",
    "scenario's nader uitgewerkt:\n",
    "Een gedistribueerd communicatienetwerk voor (geprotocolleerde)\n",
    "overdracht van gegevens door één-op-één communicatie tussen\n",
    "zorgverleners én om data beschikbaar te kunnen stellen voor andere\n",
    "doeleinden.\n",
    "Een data-centrische oplossing, bestaande uit gekoppelde dataplatfor-\n",
    "men, die het gebruik van data scheidt van de functionaliteit en\n",
    "databeschikbaarheid voor primair en secundair gebruik, alsmede\n",
    "gezamenlijke dossiervorming in de context van netwerkzorg, faciliteert.\n",
    "\n",
    "De uitwerking heeft inzichtelijk gemaakt waar de knelpunten zitten en\n",
    "welke interventies nodig zijn om tot landelijke gegevensuitwisseling en\n",
    "databeschikbaarheid te komen. Vervolgens zijn de zorgkoepels en\n",
    "adviesorganen geconsulteerd over voorgestelde aanscherpingen op de\n",
    "geadviseerde scenario's.\n",
    "Deze aanscherpingen zijn:\n",
    "\n",
    "\n",
    "\n",
    "6\n",
    "Nederlandse Federatie van Universitair Medische Centra (NFU), Nederlandse Vereniging van\n",
    "Ziekenhuizen (NVZ), Santeon en mProve\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tweede Kamer, vergaderjaar 2023–2024, 27 529, nr. 313 4\n",
    "Communicatienetwerk\n",
    "Om binnen de IZA-termijn te komen tot landelijke gegevensuit-\n",
    "wisseling wordt gekozen voor een hybride oplossing waarbij\n",
    "bestaande infrastructuren worden verbonden met elkaar en met\n",
    "zorgaanbieders die geen onderdeel uitmaken van een specifieke\n",
    "infrastructuur, maar een eigen knooppunt hebben.\n",
    "o Verbinden van bestaande infrastructuren: mijn ministerie zorgt voor\n",
    "het opstellen van «technical agreements» (TA's) per uitwisselings-\n",
    "vorm, waarmee bestaande infrastructuren met knooppunten\n",
    "verbonden worden. Hiermee zullen (leveranciers van) zorgaanbie-\n",
    "ders van bijvoorbeeld ziekenhuiszorg, wijkverpleging en verpleeg-\n",
    "zorg, de verbinding van infrastructuren beproeven en vervolgens in\n",
    "gebruik nemen. De overdracht vindt plaats op basis van FHIR en\n",
    "maakt gebruik van huidige landelijke afsprakenstelsels (zoals\n",
    "MedMij, Twiin en Health-RI) en het TwiinxNuts groeipad dat richting\n",
    "2025 wordt ontwikkeld onder het Landelijk vertrouwensstelsel;\n",
    "o Infrastructuur voor uitwisseling van medische beelden: de DVD-exit\n",
    "infrastructuur (het Twiin portaal) wordt ingezet als tijdelijke\n",
    "oplossing, inclusief lokalisatiefunctie voor de benodigde historische\n",
    "zorgtijdslijn.\n",
    "\n",
    "Data- en integratieplatform(en)\n",
    "Het uitgangspunt voor dataopslag is dat data wordt opgeslagen onder\n",
    "verantwoordelijkheid en invloedssfeer van de zorgaanbieder.\n",
    "o Hoe de data opgeslagen wordt (bronsysteem of datawarehouse/\n",
    "data lake/dataplatform) en waar de vertaling naar landelijke\n",
    "informatiestandaarden plaats vindt, is afhankelijk van de mogelijk-\n",
    "heden en keuzes van de zorgaanbieder (en haar leverancier);\n",
    "o Alleen centrale opslag van data (data van meerdere zorgaanbieders\n",
    "opgeslagen op één en dezelfde locatie) wanneer de noodzaak\n",
    "aangetoond is en er een juridische grondslag vanuit de Algemene\n",
    "Verordening Gegevensbescherming (AVG) en de Wet op de\n",
    "geneeskundige behandelovereenkomst (Wgbo) voor aanwezig is.\n",
    "Inzetten op integratie- en netwerkdiensten om versnelling te realiseren.\n",
    "o Kom tot een – op open internationale standaarden gebaseerde –\n",
    "landelijke data infrastructuur voor primair en secundair gebruik,\n",
    "waarbij het CumuluZ-concept als uitgangspunt dient voor een\n",
    "non-concurrentiële data-integratie laag;\n",
    "o Groei toe naar een publieke voorziening voor data-integratie laag\n",
    "met daarbij de benodigde integratie en netwerkdiensten, die\n",
    "bijdraagt aan de realisatie van databeschikbaarheid voor het hele\n",
    "gezondheidsinformatiestelsel;\n",
    "o Maak (her)gebruik van de kennis, ervaring en functionaliteit die\n",
    "aanwezig is bij de bewezen initiatieven en werk actief samen om\n",
    "die initiatieven te harmoniseren en uiteindelijk te integreren in één\n",
    "infrastructuur;\n",
    "o Start met regionale en sectorale initiatieven die aantoonbaar\n",
    "schaalbaar zijn en ga die na een succesvolle beproeving landelijk\n",
    "implementeren;\n",
    "o Investeer alleen in nieuwe initiatieven als die nieuwe functionaliteit\n",
    "voor de zorgsector toevoegen en passen in de beoogde\n",
    "doelarchitectuur;\n",
    "o Integratie- en netwerkdiensten zijn ter aanvulling en niet ter\n",
    "vervanging van de benodigde eenheid van taal en techniek. Vanuit\n",
    "kwaliteit, verantwoordelijkheid en kosten perspectief blijft gestan-\n",
    "daardiseerde vastlegging de verantwoordelijkheid van de zorgaan-\n",
    "bieder (die hierin gefaciliteerd moet worden door de leverancier\n",
    "van het bronsysteem) en dient dit zo dicht mogelijk bij de bron\n",
    "(zorgverlener) plaats te vinden.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tweede Kamer, vergaderjaar 2023–2024, 27 529, nr. 313 5\n",
    "De leden van het Informatieberaad zorg hebben hun steun uitgesproken\n",
    "voor deze koers met de aanscherpingen op de twee scenario's. Dit moet\n",
    "de komende maanden leiden tot een verbreding van de CumuluZ-coalitie,\n",
    "met een vertegenwoordiging uit meerdere zorgdomeinen en het inregelen\n",
    "van de governance van de coalitie (incl. juridische entiteit en rol van het\n",
    "Ministerie van VWS). Daarnaast wil ik een «werkplaats» inrichten om niet\n",
    "alleen op bestuurlijk niveau (CumuluZ-coalitie), maar vooral ook op\n",
    "inhoud partijen actief samen te laten werken om proeftuinen te initiëren\n",
    "en bestaande, bewezen toepassingen te harmoniseren en integreren in de\n",
    "doelarchitectuur.\n",
    "\n",
    "Vervolg\n",
    "\n",
    "Ik zet de huidige koers van standaardisatie van taal en techniek voort, die\n",
    "nodig is voor in eerste instantie gegevensuitwisseling groeiend naar\n",
    "databeschikbaarheid. Zoals ook benoemd in de kamerbrief over Generieke\n",
    "functies is het noodzakelijk om te komen tot een landelijke kaderstelling,\n",
    "in de vorm van een landelijk vertrouwensstelsel.\n",
    "\n",
    "In aanvulling hierop ga ik samen met de CumuluZ-coalitie de regie voeren\n",
    "op het realiseren van een landelijke data infrastructuur voor primair en\n",
    "secundair gebruik. Daarbij wil ik toegroeien naar een publieke voorziening\n",
    "voor de data-integratie laag, met bijbehorende integratie- en netwerk-\n",
    "diensten, om de doorontwikkeling van gegevensuitwisseling naar\n",
    "databeschikbaarheid te ondersteunen en versnellen. De mogelijkheid,\n",
    "intensiteit en snelheid van deze ambitie moeten worden bezien in\n",
    "samenhang met de middelen die beschikbaar zijn op de aanvullende post\n",
    "bij het Ministerie van Financiën voor het bevorderen van de gegevensuit-\n",
    "wisseling in de zorg. Over de inzet van deze middelen moet nadere\n",
    "besluitvorming plaatsvinden. Ik bericht u na het voorjaar over de stand\n",
    "van zaken.\n",
    "\n",
    "De volgende stap is het uitwerken van de aangescherpte scenario's in een\n",
    "doelarchitectuur en een transitieplan, met concrete acties voor de korte\n",
    "termijn en langere termijn doelen voor de daaropvolgende jaren. Hierbij\n",
    "wordt aangesloten op de besluiten die in het IZA-uitvoeringsakkoord zijn\n",
    "opgenomen en de doelstelling (per plateau) van de NVS. De regie ligt bij\n",
    "mijn ministerie, en de invulling van het plan zal in afstemming met de\n",
    "CumuluZ-coalitie en IZA-partijen plaatsvinden. Voor het einde van dit jaar\n",
    "zal ik uw kamer informeren over de voortgang omtrent de doelstellingen\n",
    "die in 2025 gerealiseerd moeten worden en de doelarchitectuur en het\n",
    "transitieplan.\n",
    "\n",
    "De Minister van Volksgezondheid, Welzijn en Sport,\n",
    "C. Helder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tweede Kamer, vergaderjaar 2023–2024, 27 529, nr. 313 6'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pypdf\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader(\"LandelijkDekkendNetwerk.pdf\")\n",
    "docs = pdf_loader.load()\n",
    "pages = pdf_loader.load_and_split()\n",
    "print(len(pages))\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 2\n",
    "list_page_content = [page.page_content for page in pages]\n",
    "some_text = ''.join(list_page_content)\n",
    "ntchar = len(some_text)\n",
    "chunk_size = int(ntchar/n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=0,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = r_splitter.split_text(some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. Text to PDF\n",
    "Sections are:\n",
    "1) Kunstmatige intelligentie\n",
    "2) History Greek, Dutch and Belgium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1. Kunstmatige Intelligentie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet1 = \"Medewerkers gaan we verder ontwikkelen in het toepassen van de menselijke maat in onze dienstverlening. Met ondersteuning van kunstmatige intelligentie gaan we onze brieven leesbaarder en begrijpelijker maken. De benadering is van buiten naar binnen: knelpunten die onze cliënten ervaren worden in kaart gebracht op basis van verschillende vormen van (klant)onderzoek en analyses.\"\n",
    "snippet2 = \"Het programma Innovatie ondersteunt initiatieven en oplosteams in het effectief organiseren van verbetertrajecten en het bedenken van vernieuwende oplossingen, met kennis over de laatste (technologische) ontwikkelingen en trends. Zo wordt onderzocht hoe kunstmatige intelligentie (zoals ChatGPT) ingezet kan worden, bijvoorbeeld bij het herschrijven van algemene teksten in tientallen brieven om de leesbaarheid te verbeteren. Aandacht voor innovatie en design thinking draagt ook bij aan de gewenste ontwikkeling van een lerende organisatie.\"\n",
    "snippet_collection_nl = snippet1 + \"\\n\" + snippet2\n",
    "\n",
    "snippet_collection_en = '''We will further develop employees in applying the human scale in our services. With support of\n",
    "artificial intelligence we will make our letters more readable and make it more understandable. The approach is from the outside in: bottlenecks\n",
    "that our clients experience are mapped on the basis of various forms of (customer) research and analyses. The program\n",
    "Innovation supports initiatives and solution teams effectively organizing improvement processes and devising innovative ones\n",
    "solutions, with knowledge of the latest (technological) developments and trends. For example, it is investigated how artificial intelligence (such as\n",
    "ChatGPT) can be used, for example when rewriting general texts in dozens of letters to improve readability.\n",
    "Attention to innovation and design thinking also contributes to the desired results development of a learning organization.'''\n",
    "create_pdf([snippet_collection_nl], \"KI_nl.pdf\")\n",
    "create_pdf([snippet_collection_en], \"KI_en.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2. History Greek, Dutch and Belgium\n",
    "Depending on the input text of the be, nl, gr fragments the map reduce gives different output. That is why i am creating two versions of the pdfs. The first version is based on the text1 fragments and the second version is based on the text2 fragments. Notice the difference between the text1 and text2 fragments. Text1 uses \\n whereas text2 versions use cariage returns leading to white lines between alineas. Comparing the history1 and history2 pdfs you can not see any difference although the output of the Map-Reduce is highly different. See Map-Reduce output under __Useful Code__ section 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "be_history_text1 = '''Belgian history is rich and complex, characterized by its strategic location in Western Europe and \\n\n",
    "its cultural diversity. Here's a brief summary:\n",
    "Early History: The region now known as Belgium has been inhabited since prehistoric times. \\n\n",
    "It was later settled by Celtic and Germanic tribes before coming under Roman rule in the first century BC. The area flourished during Roman times as part of the province of Gallia Belgica.\n",
    "Medieval Period: After the fall of the Roman Empire, the region was invaded and settled by various Germanic tribes. In the early Middle Ages, it became part of the Frankish Empire. During this period, the area saw the rise of powerful feudal lords and the emergence of important trading cities like Ghent, Bruges, and Antwerp.\n",
    "Burgundian and Habsburg Rule: In the 15th century, the Burgundian dukes gained control of much of present-day Belgium. This period saw the flourishing of arts and culture, but also increased centralization of power. The region later came under Habsburg rule as part of the Spanish and Austrian Netherlands.\n",
    "Dutch Independence: In the 16th and 17th centuries, the Dutch Revolt against Spanish rule led to the independence of the northern provinces of the Netherlands. However, the southern provinces, including present-day Belgium, remained under Spanish control until they were conquered by France in the late 17th century.\n",
    "French Rule: Belgium became part of France under Napoleon Bonaparte's rule in the early 19th century. During this time, French revolutionary ideals influenced Belgian society and politics.Independence and Kingdom of Belgium: Following the defeat of Napoleon, the Congress of Vienna in 1815 united the southern provinces with the northern provinces to form the United Kingdom of the Netherlands. However, tensions between the Dutch-speaking north and the French-speaking south led to the Belgian Revolution in 1830. Belgium declared independence and established a constitutional monarchy, with Leopold I as its first king.\n",
    "Industrialization and Colonialism: Throughout the 19th century, Belgium experienced rapid industrialization, particularly in coal mining and steel production. It also established a colonial empire in Africa, notably in the Congo, which was famously exploited under King Leopold II's rule.\n",
    "20th Century: Belgium was heavily impacted by both World Wars, particularly during World War I when it served as a battleground. The country was occupied by Germany during World War II. After the war, Belgium played a key role in the founding of the European Coal and Steel Community, a precursor to the European Union.\n",
    "Modern Belgium: Belgium has since become a prosperous and democratic country, known for its multiculturalism, chocolate, beer, and waffles. However, it continues to grapple with linguistic and political tensions between the Dutch-speaking Flanders region and the French-speaking Wallonia region, as well as issues related to regional autonomy and identity.'''\n",
    "\n",
    "nl_history_text1 = '''The history of the Netherlands is rich and diverse, spanning thousands of years. Here's a brief summary of key periods and events in Dutch history:\n",
    "Early Settlements: The region that is now the Netherlands has been inhabited since prehistoric times. During the Roman era, it was part of the Roman Empire's frontier region.\n",
    "Middle Ages: In the early Middle Ages, the Franks established control over the region. The Netherlands gradually emerged as a distinct entity, with the development of feudal states and the growth of trade and commerce.\n",
    "Golden Age (17th Century): The 17th century is often referred to as the Dutch Golden Age. During this time, the Netherlands experienced a period of economic prosperity, cultural flourishing, and naval dominance. The Dutch East India Company and Dutch West India Company were established, and Amsterdam became a leading financial center.\n",
    "Colonial Empire: The Dutch established colonies and trading posts around the world, including in the East Indies (present-day Indonesia), Suriname, and the Caribbean. The Dutch colonial empire was significant but eventually declined over time.\n",
    "Napoleonic Era: In the late 18th and early 19th centuries, the Netherlands fell under French control during the Napoleonic Wars. It later became part of the French Empire.\n",
    "Independence and Kingdom: The Netherlands gained independence from France in 1815 and became a kingdom under King William I. Belgium initially formed part of the Kingdom of the Netherlands but later separated in 1830.\n",
    "Industrialization and Modernization: The 19th century saw rapid industrialization and modernization in the Netherlands. The country became known for its innovations in trade, shipping, and agriculture.\n",
    "World Wars: The Netherlands remained neutral during World War I but was invaded by Nazi Germany in World War II. The country suffered under German occupation but played a role in the Allied liberation of Europe.\n",
    "Post-War Reconstruction: After World War II, the Netherlands underwent a period of reconstruction and economic recovery. It became a founding member of international organizations such as the United Nations and the European Union.\n",
    "Contemporary Era: In recent decades, the Netherlands has become known for its progressive social policies, strong economy, and commitment to environmental sustainability. It continues to be a leading global player in areas such as trade, technology, and diplomacy.\n",
    "This summary provides a broad overview of Dutch history, highlighting key moments and themes that have shaped the nation's identity and development over time.'''\n",
    "\n",
    "gr_history_text1 = '''Greek history spans thousands of years and is marked by significant contributions to Western civilization, including democracy, philosophy, art, and literature. Here's a brief summary:\n",
    "Ancient Greece: Ancient Greek civilization emerged around the 8th century BC and was comprised of city-states such as Athens, Sparta, Corinth, and Thebes. This period saw the rise of democracy in Athens, where citizens participated in governance, and the development of philosophy by figures like Socrates, Plato, and Aristotle. Greek art and architecture, exemplified by the Parthenon in Athens, also flourished during this time. The city-states often engaged in conflicts with each other, most notably the Peloponnesian War between Athens and Sparta.\n",
    "Hellenistic Period: After the conquests of Alexander the Great in the 4th century BC, Greek culture spread throughout the Mediterranean and Middle East, creating a new era known as the Hellenistic period. Greek language, art, and philosophy influenced cultures across the region, including Egypt and Persia.\n",
    "Roman Greece: Greece became part of the Roman Empire after the defeat of the Greek city-states in the 2nd century BC. During this time, Greece continued to be an important center of culture and learning, with cities like Corinth and Athens remaining influential.\n",
    "Byzantine Empire: Following the division of the Roman Empire, Greece became part of the Byzantine Empire, centered in Constantinople (modern-day Istanbul). The Byzantine period saw the spread of Christianity and the construction of numerous churches and monasteries across Greece.\n",
    "Ottoman Rule: Greece fell under Ottoman rule in the 15th century after the fall of Constantinople. The Greeks struggled for independence from Ottoman rule for centuries, culminating in the Greek War of Independence in the early 19th century.\n",
    "Modern Greece: The Greek War of Independence began in 1821 and eventually led to the establishment of the modern Greek state in 1830, although some territories, including Crete and the Ionian Islands, were not incorporated until later. The monarchy was established, and Otto of Bavaria became the first king of Greece.\n",
    "20th Century: Greece experienced political instability throughout much of the 20th century, including periods of monarchy, dictatorship, and democratic rule. Greece was occupied by Axis powers during World War II, and a brutal civil war followed the war's end. In 1974, Greece transitioned to democracy after the fall of the military junta.\n",
    "European Union and Economic Challenges: Greece joined the European Union in 1981 and adopted the euro as its currency in 2001. However, the country faced significant economic challenges in the late 2000s, leading to a sovereign debt crisis and bailout agreements with the EU and International Monetary Fund.\n",
    "Modern Greece: Today, Greece is a parliamentary republic and a member of the European Union. It remains a popular tourist destination known for its rich history, stunning landscapes, and cultural heritage. However, it continues to grapple with economic issues and challenges related to governance and social welfare. '''\n",
    "\n",
    "\n",
    "be_history_text2 = '''Belgian history is rich and complex, characterized by its strategic location in Western Europe and its cultural diversity. Here's a brief summary:\n",
    "\n",
    "Early History: The region now known as Belgium has been inhabited since prehistoric times. It was later settled by Celtic and Germanic tribes before coming under Roman rule in the first century BC. The area flourished during Roman times as part of the province of Gallia Belgica.\n",
    "\n",
    "Medieval Period: After the fall of the Roman Empire, the region was invaded and settled by various Germanic tribes. In the early Middle Ages, it became part of the Frankish Empire. During this period, the area saw the rise of powerful feudal lords and the emergence of important trading cities like Ghent, Bruges, and Antwerp.\n",
    "\n",
    "Burgundian and Habsburg Rule: In the 15th century, the Burgundian dukes gained control of much of present-day Belgium. This period saw the flourishing of arts and culture, but also increased centralization of power. The region later came under Habsburg rule as part of the Spanish and Austrian Netherlands.\n",
    "\n",
    "Dutch Independence: In the 16th and 17th centuries, the Dutch Revolt against Spanish rule led to the independence of the northern provinces of the Netherlands. However, the southern provinces, including present-day Belgium, remained under Spanish control until they were conquered by France in the late 17th century.\n",
    "\n",
    "French Rule: Belgium became part of France under Napoleon Bonaparte's rule in the early 19th century. During this time, French revolutionary ideals influenced Belgian society and politics.Independence and Kingdom of Belgium: Following the defeat of Napoleon, the Congress of Vienna in 1815 united the southern provinces with the northern provinces to form the United Kingdom of the Netherlands. However, tensions between the Dutch-speaking north and the French-speaking south led to the Belgian Revolution in 1830. Belgium declared independence and established a constitutional monarchy, with Leopold I as its first king.\n",
    "\n",
    "Industrialization and Colonialism: Throughout the 19th century, Belgium experienced rapid industrialization, particularly in coal mining and steel production. It also established a colonial empire in Africa, notably in the Congo, which was famously exploited under King Leopold II's rule.\n",
    "\n",
    "20th Century: Belgium was heavily impacted by both World Wars, particularly during World War I when it served as a battleground. The country was occupied by Germany during World War II. After the war, Belgium played a key role in the founding of the European Coal and Steel Community, a precursor to the European Union.\n",
    "\n",
    "Modern Belgium: Belgium has since become a prosperous and democratic country, known for its multiculturalism, chocolate, beer, and waffles. However, it continues to grapple with linguistic and political tensions between the Dutch-speaking Flanders region and the French-speaking Wallonia region, as well as issues related to regional autonomy and identity. '''\n",
    "\n",
    "gr_history_text2 = '''Greek history spans thousands of years and is marked by significant contributions to Western civilization, including democracy, philosophy, art, and literature. Here's a brief summary:\n",
    "\n",
    "Ancient Greece: Ancient Greek civilization emerged around the 8th century BC and was comprised of city-states such as Athens, Sparta, Corinth, and Thebes. This period saw the rise of democracy in Athens, where citizens participated in governance, and the development of philosophy by figures like Socrates, Plato, and Aristotle. Greek art and architecture, exemplified by the Parthenon in Athens, also flourished during this time. The city-states often engaged in conflicts with each other, most notably the Peloponnesian War between Athens and Sparta.\n",
    "\n",
    "Hellenistic Period: After the conquests of Alexander the Great in the 4th century BC, Greek culture spread throughout the Mediterranean and Middle East, creating a new era known as the Hellenistic period. Greek language, art, and philosophy influenced cultures across the region, including Egypt and Persia.\n",
    "\n",
    "Roman Greece: Greece became part of the Roman Empire after the defeat of the Greek city-states in the 2nd century BC. During this time, Greece continued to be an important center of culture and learning, with cities like Corinth and Athens remaining influential.\n",
    "\n",
    "Byzantine Empire: Following the division of the Roman Empire, Greece became part of the Byzantine Empire, centered in Constantinople (modern-day Istanbul). The Byzantine period saw the spread of Christianity and the construction of numerous churches and monasteries across Greece.\n",
    "\n",
    "Ottoman Rule: Greece fell under Ottoman rule in the 15th century after the fall of Constantinople. The Greeks struggled for independence from Ottoman rule for centuries, culminating in the Greek War of Independence in the early 19th century.\n",
    "\n",
    "Modern Greece: The Greek War of Independence began in 1821 and eventually led to the establishment of the modern Greek state in 1830, although some territories, including Crete and the Ionian Islands, were not incorporated until later. The monarchy was established, and Otto of Bavaria became the first king of Greece.\n",
    "\n",
    "20th Century: Greece experienced political instability throughout much of the 20th century, including periods of monarchy, dictatorship, and democratic rule. Greece was occupied by Axis powers during World War II, and a brutal civil war followed the war's end. In 1974, Greece transitioned to democracy after the fall of the military junta.\n",
    "\n",
    "European Union and Economic Challenges: Greece joined the European Union in 1981 and adopted the euro as its currency in 2001. However, the country faced significant economic challenges in the late 2000s, leading to a sovereign debt crisis and bailout agreements with the EU and International Monetary Fund.\n",
    "\n",
    "Modern Greece: Today, Greece is a parliamentary republic and a member of the European Union. It remains a popular tourist destination known for its rich history, stunning landscapes, and cultural heritage. However, it continues to grapple with economic issues and challenges related to governance and social welfare. '''\n",
    "\n",
    "nl_history_text2 = '''The history of the Netherlands is rich and diverse, spanning thousands of years. Here's a brief summary of key periods and events in Dutch history:\n",
    "\n",
    "Early Settlements: The region that is now the Netherlands has been inhabited since prehistoric times. During the Roman era, it was part of the Roman Empire's frontier region.\n",
    "\n",
    "Middle Ages: In the early Middle Ages, the Franks established control over the region. The Netherlands gradually emerged as a distinct entity, with the development of feudal states and the growth of trade and commerce.\n",
    "\n",
    "Golden Age (17th Century): The 17th century is often referred to as the Dutch Golden Age. During this time, the Netherlands experienced a period of economic prosperity, cultural flourishing, and naval dominance. The Dutch East India Company and Dutch West India Company were established, and Amsterdam became a leading financial center.\n",
    "\n",
    "Colonial Empire: The Dutch established colonies and trading posts around the world, including in the East Indies (present-day Indonesia), Suriname, and the Caribbean. The Dutch colonial empire was significant but eventually declined over time.\n",
    "\n",
    "Napoleonic Era: In the late 18th and early 19th centuries, the Netherlands fell under French control during the Napoleonic Wars. It later became part of the French Empire.\n",
    "\n",
    "Independence and Kingdom: The Netherlands gained independence from France in 1815 and became a kingdom under King William I. Belgium initially formed part of the Kingdom of the Netherlands but later separated in 1830.\n",
    "\n",
    "Industrialization and Modernization: The 19th century saw rapid industrialization and modernization in the Netherlands. The country became known for its innovations in trade, shipping, and agriculture.\n",
    "\n",
    "World Wars: The Netherlands remained neutral during World War I but was invaded by Nazi Germany in World War II. The country suffered under German occupation but played a role in the Allied liberation of Europe.\n",
    "\n",
    "Post-War Reconstruction: After World War II, the Netherlands underwent a period of reconstruction and economic recovery. It became a founding member of international organizations such as the United Nations and the European Union.\n",
    "\n",
    "Contemporary Era: In recent decades, the Netherlands has become known for its progressive social policies, strong economy, and commitment to environmental sustainability. It continues to be a leading global player in areas such as trade, technology, and diplomacy.\n",
    "\n",
    "This summary provides a broad overview of Dutch history, highlighting key moments and themes that have shaped the nation's identity and development over time. '''\n",
    "\n",
    "print(f\"Number of words in the History texts:\\n(Gr, Nl, Be) = ({len(gr_history_text.split(' '))}, {len(nl_history_text.split(' '))}, {len(be_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First page about belgium, second about grece\n",
    "filename = \"history1_be_gr_nl.pdf\"\n",
    "create_pdf([be_history_text1, gr_history_text1, nl_history_text1], filename)\n",
    "# First page about grece, second about belgium\n",
    "filename = \"history1_gr_be_nl.pdf\"\n",
    "create_pdf([gr_history_text1, be_history_text1, nl_history_text1], filename)\n",
    "\n",
    "filename = \"history2_be_gr_nl.pdf\"\n",
    "create_pdf([be_history_text2, gr_history_text2, nl_history_text2], filename)\n",
    "# First page about grece, second about belgium\n",
    "filename = \"history2_gr_be_nl.pdf\"\n",
    "create_pdf([gr_history_text2, be_history_text2, nl_history_text2], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Code:\n",
    "1) Loading a pdf file\n",
    "2) __Selects__ from a given text, all the __sentences__ given between __start_index__ and __end_index__.\n",
    "3) Code and example how to use the __generation of a pdf__ based on a list of input strings.\n",
    "4) Code to create a pandas __dataframe__ from the __LangChain map-reduce__ outputs\n",
    "5) Code to load the Llama model (dutch or english) using __pipeline__ from __transformers__ (as present in the demo given to Casper)\n",
    "6) Code to load the Llama model (dutch or english) using __HuggingFaceHub__ (errors due to modelsize >)\n",
    "7) Code to upload the Llama model to Hugging Face Space (although suggested by the output error it seems not to be possible) \n",
    "8) Code to upload the Llama model using __AutoModelForCausalLM__ and using the __AutoTokenizer__ and __pipeline__ from transformers wrapping everything in __HuggingFacePipeline__ that can be used by LangChain (This approach was Successfull)\n",
    "9) __Cpp__ way of loading the model \n",
    "10) Login to hugging face\n",
    "11) Map-Reduce stress test: Map: \"Extract nth sentence of the document splits\" and Reduce: \"concatenate them into final output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Loading a pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"KI_nl.pdf\")\n",
    "docs = loader.load()\n",
    "print(f\"Number of docs: {len(docs)}\\n===============\\n\")\n",
    "print(f\"page_content of first page:\\n===========================\\n{docs[0].page_content}\\n\\n\")\n",
    "print(f\"metadata:\\n=========\\n{docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2. Selects from a given text, all the sentences given between start_index and end_index. \n",
    "- __select_sentences_range(text, 3, 6)__ --> will give you from the input text all the sentences from sentence 3 till sentence 6\n",
    "- __select_sentences_range(text, 3,\":\")__ --> will give you all the sentences from the text from the 3rd sentence till the end of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def select_sentences_range(text, start_index, end_index):\n",
    "    # Split the text into sentences using regex\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    \n",
    "    # Convert \":\" to the length of the sentences list\n",
    "    if end_index == \":\":\n",
    "        end_index = len(sentences)\n",
    "    \n",
    "    # Select sentences within the specified range\n",
    "    selected_sentences = sentences[start_index:end_index]\n",
    "    \n",
    "    # Join the selected sentences into a single string\n",
    "    selected_text = ' '.join(selected_sentences)\n",
    "    \n",
    "    return selected_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 3. Code and example how to use the generation of a pdf based on a list of input strings. Each string of the list will be on its own page in the pdf document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install reportlab\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "def create_pdf(list_strings, filename):\n",
    "    # Create a canvas with letter size (8.5x11 inches)\n",
    "    c = canvas.Canvas(filename, pagesize=letter)\n",
    "\n",
    "    # Set font and font size for first page\n",
    "    c.setFont(\"Helvetica\", 12)\n",
    "    \n",
    "    for string in list_strings:\n",
    "        # Draw first string on the first page\n",
    "        draw_multiline_string(c, 100, 700, string)\n",
    "\n",
    "        # Add a new page\n",
    "        c.showPage()\n",
    "\n",
    "    # Save the PDF\n",
    "    c.save()\n",
    "\n",
    "def draw_multiline_string(canvas, x, y, text, max_width=400, line_spacing=15):\n",
    "    lines = []\n",
    "    current_line = ''\n",
    "    for word in text.split():\n",
    "        if canvas.stringWidth(current_line + ' ' + word) <= max_width:\n",
    "            current_line += ' ' + word\n",
    "        else:\n",
    "            lines.append(current_line.strip())\n",
    "            current_line = word\n",
    "    lines.append(current_line.strip())\n",
    "\n",
    "    for line in lines:\n",
    "        canvas.drawString(x, y, line)\n",
    "        y -= line_spacing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet1 = \"Medewerkers gaan we verder ontwikkelen in het toepassen van de menselijke maat in onze dienstverlening. Met ondersteuning van kunstmatige intelligentie gaan we onze brieven leesbaarder en begrijpelijker maken. De benadering is van buiten naar binnen: knelpunten die onze cliënten ervaren worden in kaart gebracht op basis van verschillende vormen van (klant)onderzoek en analyses.\"\n",
    "snippet2 = \"Het programma Innovatie ondersteunt initiatieven en oplosteams in het effectief organiseren van verbetertrajecten en het bedenken van vernieuwende oplossingen, met kennis over de laatste (technologische) ontwikkelingen en trends. Zo wordt onderzocht hoe kunstmatige intelligentie (zoals ChatGPT) ingezet kan worden, bijvoorbeeld bij het herschrijven van algemene teksten in tientallen brieven om de leesbaarheid te verbeteren. Aandacht voor innovatie en design thinking draagt ook bij aan de gewenste ontwikkeling van een lerende organisatie.\"\n",
    "snippet_collection_nl = snippet1 + \"\\n\" + snippet2\n",
    "\n",
    "snippet_collection_en = '''We will further develop employees in applying the human scale in our services. With support of\n",
    "artificial intelligence we will make our letters more readable and make it more understandable. The approach is from the outside in: bottlenecks\n",
    "that our clients experience are mapped on the basis of various forms of (customer) research and analyses. The program\n",
    "Innovation supports initiatives and solution teams effectively organizing improvement processes and devising innovative ones\n",
    "solutions, with knowledge of the latest (technological) developments and trends. For example, it is investigated how artificial intelligence (such as\n",
    "ChatGPT) can be used, for example when rewriting general texts in dozens of letters to improve readability.\n",
    "Attention to innovation and design thinking also contributes to the desired results development of a learning organization.'''\n",
    "create_pdf([snippet_collection_nl], \"KI_nl.pdf\")\n",
    "create_pdf([snippet_collection_en], \"KI_en.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 4. Code to create a pandas dataframe from the LangChain map-reduce outputs¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas\n",
    "from pathlib import Path as p\n",
    "import pandas as pd\n",
    "\n",
    "def create_pd_from_map_reduce_output(map_reduce_outputs):\n",
    "    data_folder = p.cwd() \n",
    "    p(data_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    final_mp_data = []\n",
    "    for doc, out in zip(map_reduce_outputs[\"input_documents\"], map_reduce_outputs[\"intermediate_steps\"]):\n",
    "        output = {}\n",
    "        output[\"file_name\"] = p(doc.metadata[\"source\"]).stem\n",
    "        output[\"file_type\"] = p(doc.metadata[\"source\"]).suffix\n",
    "        output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "        output[\"chunks\"] = doc.page_content\n",
    "        output[\"concise_summary\"] = out\n",
    "        final_mp_data.append(output)\n",
    "\n",
    "    pdf_mp_summary = pd.DataFrame.from_dict(final_mp_data)\n",
    "    pdf_mp_summary = pdf_mp_summary.sort_values(by=[\"file_name\", \"page_number\"])  # sorting the dataframe by filename and page_number\n",
    "    pdf_mp_summary.reset_index(inplace=True, drop=True)\n",
    "    pdf_mp_summary\n",
    "    return pdf_mp_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that prints the output in a human readable format \n",
    "for i in range(len(pdf_mp_summary)):\n",
    "   print(f\"\\n ========================================\\n{pdf_mp_summary.iloc[i]['chunks']} \\n+++++++++++++++++++++++\\n{pdf_mp_summary.iloc[i]['concise_summary']}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 5. Code to load the Llama model (dutch or english) using the __pipeline__ from __transformers__ (as present in the demo given to Casper)\n",
    "- set in the below cell the parameter __model_chosen_id__ either to __=1__ (english model) or to __=2__ (dutch model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: meta-llama/Llama-2-13b-chat-hf\n",
      "Parameters are: {'model_name': 'meta-llama/Llama-2-13b-chat-hf', 'do_sample': True, 'temperature': 0.1, 'repetition_penalty': 1.1, 'max_new_tokens': 500}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c564b340d7f041169a45a116bdff44c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model: 19.436502933502197 seconds\n"
     ]
    }
   ],
   "source": [
    "#!pip install langdetect\n",
    "import re\n",
    "import time\n",
    "from transformers import pipeline, Conversation, AutoTokenizer\n",
    "from langdetect import detect\n",
    "\n",
    "# choose your model here by setting model_chosen_id equal to 1 or 2\n",
    "model_chosen_id = 1\n",
    "model_name_options = {\n",
    "    1: \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "    2: \"BramVanroy/Llama-2-13b-chat-dutch\"\n",
    "}\n",
    "model_chosen = model_name_options[model_chosen_id]\n",
    "\n",
    "my_config = {'model_name': model_chosen, 'do_sample': True, 'temperature': 0.1, 'repetition_penalty': 1.1, 'max_new_tokens': 500, }\n",
    "print(f\"Selected model: {my_config['model_name']}\")\n",
    "print(f\"Parameters are: {my_config}\")\n",
    "\n",
    "def count_words(text):\n",
    "    # Use a simple regular expression to count words\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return len(words)\n",
    "\n",
    "def generate_with_llama_chat(my_config):    \n",
    "    # get the parameters from the config dict\n",
    "    do_sample = my_config.get('do_sample', True)\n",
    "    temperature = my_config.get('temperature', 0.1)\n",
    "    repetition_penalty = my_config.get('repetition_penalty', 1.1)\n",
    "    max_new_tokens = my_config.get('max_new_tokens', 500)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = my_config['model_name']\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    \n",
    "    # Language code for Dutch\n",
    "    #lang_code = \"nl_XX\"\n",
    "    #forced_bos_token_id = tokenizer.lang_code_to_id[\"nl_XX\"] # Error lang_code_to_id not know\n",
    "    \n",
    "    #potential usful parameters to tweak: ,\"do_sample\": True, \"max_lengt\n",
    "    chatbot = pipeline(\"conversational\",model=model, \n",
    "                       tokenizer=tokenizer,\n",
    "                       do_sample=do_sample, \n",
    "                       temperature=temperature, \n",
    "                       repetition_penalty=repetition_penalty,\n",
    "                       #max_length=2000,\n",
    "                       max_new_tokens=max_new_tokens, \n",
    "                       model_kwargs={\"device_map\": \"auto\",\"load_in_8bit\": True})  #, \"src_lang\": \"en\", \"tgt_lang\": \"nl\"})  does not work!\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Loading the model: {elapsed_time} seconds\")\n",
    "    return chatbot\n",
    "    \n",
    "def get_answer(chatbot, input_text):\n",
    "    start_time = time.time()\n",
    "    print(f\"Processing the input\\n {input_text}\\n\")\n",
    "    print('Processing the answer....')\n",
    "    conversation = Conversation(input_text)\n",
    "    print(f\"Conversation(input_text): {conversation}\")\n",
    "    output = (chatbot(conversation))[1]['content']\n",
    "    output_language = detect(output)\n",
    "    print(f\"{output}\\n\")\n",
    "    print(f\"output language detected is {output_language}\\n\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Answered in {elapsed_time:.1f} seconds, Nr generated words: {count_words(output)}\\n\")\n",
    "\n",
    "    # Perform translation to dutch (catch in case it is needed (prompt engineering does not always works)\n",
    "    if output_language == 'en':\n",
    "        print(\"----------------------------------------------------\")\n",
    "        print(\"Need extra time to make the translation to Dutch....\")\n",
    "        start_time = time.time()\n",
    "        conversation = Conversation(f\"Translate the following text to Dutch: {output}\")\n",
    "        output = (chatbot(conversation))[1]['content']\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"translated output is: {output}\\n\")\n",
    "        print(f\"Translation time: {elapsed_time:.1f}, Nr generated words: {count_words(output)}\")\n",
    "\n",
    "\n",
    "chatbot = generate_with_llama_chat(my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 6. Code to load the Llama model (dutch or english) using __HuggingFaceHub__:\n",
    "- __Error:__\n",
    "  The model BramVanroy/Llama-2-13b-chat-dutch is too large to be loaded automatically (26GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).\n",
    "\n",
    "  - There are two options to circumvent this issue:\n",
    "    1) Using Hugging Face Spaces\n",
    "    2) Using Inference Endpoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "# Verify if the environment variable contains the right content\n",
    "print(os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "question = \"Who won the FIFA World Cup in the year 1994? \"\n",
    "template = \"\"\"Question: {question}\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"BramVanroy/Llama-2-13b-chat-dutch\", model_kwargs={\"temperature\": 0.5, \"max_length\": 64}\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "print(llm_chain.invoke(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 7. Code to uploading the model to my Hugging Face __Space__ (Does not work)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This cell bellow, did not work:\n",
    "- __Error:__\n",
    "- usage: huggingface-cli <command> [<args>]\n",
    "huggingface-cli: error: argument {env,login,whoami,logout,repo,upload,download,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache}: invalid choice: 'space' (choose from 'env', 'login', 'whoami', 'logout', 'repo', 'upload', 'download', 'lfs-enable-largefiles', 'lfs-multipart-upload', 'scan-cache', 'delete-cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: argument {env,login,whoami,logout,repo,upload,download,lfs-enable-largefiles,lfs-multipart-upload,scan-cache,delete-cache}: invalid choice: 'space' (choose from 'env', 'login', 'whoami', 'logout', 'repo', 'upload', 'download', 'lfs-enable-largefiles', 'lfs-multipart-upload', 'scan-cache', 'delete-cache')\n"
     ]
    }
   ],
   "source": [
    "#!pip install huggingface_hub\n",
    "#huggingface-cli login\n",
    "!huggingface-cli space push model my_BramVanroy_Dutch_model BramVanroy/Llama-2-13b-chat-dutch --organization polpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: unrecognized arguments: --organization polpo\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli upload model my_BramVanroy_Dutch_model BramVanroy/Llama-2-13b-chat-dutch --organization polpo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 8. Loading the model using __AutoModelForCausalLM__ and using the AutoTokenizer and pipeline from transformers wrapping everything in __LangChain__ pipeline (called __HuggingFacePipeline__)\n",
    "Sections are:\n",
    "1) llm that can be used with prompting\n",
    "2) General question-prompt about the EEG without input context\n",
    "3) Prompt asking to summarize the input text with __LangChain Stuff__ method\n",
    "4) Prompt asking to summarize the input text with __LangChain Map-Reduce__ method\n",
    "5) Prompt asking to summarize the input text with __LangChain Refine__ method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1. llm that can be used with prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: meta-llama/Llama-2-13b-chat-hf\n",
      "Parameters are: {'model_name': 'meta-llama/Llama-2-13b-chat-hf', 'do_sample': True, 'temperature': 0.1, 'repetition_penalty': 1.1, 'max_new_tokens': 500}\n",
      "tokenizer\n",
      "causal\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fecfe61c003c40f6aa67bf9301b23034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3d077de0e9249be8c0a76ccd8dd0002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install transformers\n",
    "#!pip install accelerate\n",
    "#!pip install bitsandbytes\n",
    "\n",
    "from transformers import pipeline, Conversation, AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "#1: \"meta-llama/Llama-2-13b-chat-hf\",\n",
    "#2: \"BramVanroy/Llama-2-13b-chat-dutch\"\n",
    "my_config = {'model_name': \"meta-llama/Llama-2-13b-chat-hf\", #\"./Bram\", #BramVanroy/Llama-2-13b-chat-dutch\", \n",
    "             'do_sample': True, 'temperature': 0.1, \n",
    "             'repetition_penalty': 1.1, 'max_new_tokens': 500, }\n",
    "\n",
    "print(f\"Selected model: {my_config['model_name']}\")\n",
    "print(f\"Parameters are: {my_config}\")\n",
    "\n",
    "def generate_with_llama_chat(my_config):\n",
    "    print('tokenizer')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(my_config['model_name'])\n",
    "    print('causal')\n",
    "    model = AutoModelForCausalLM.from_pretrained(my_config['model_name'])\n",
    "    print('Pipeline')\n",
    "    chatbot = pipeline(\"text-generation\",model=my_config['model_name'], \n",
    "                       tokenizer=tokenizer,\n",
    "                       do_sample=my_config['do_sample'], \n",
    "                       temperature=my_config['temperature'], \n",
    "                       repetition_penalty=my_config['repetition_penalty'],\n",
    "                       #max_length=my_config['max_length'],\n",
    "                       max_new_tokens=my_config['max_new_tokens'], \n",
    "                       model_kwargs={\"device_map\": \"auto\",\"load_in_8bit\": True})\n",
    "    return chatbot\n",
    "\n",
    "llama_chat = generate_with_llama_chat(my_config)\n",
    "\n",
    "# Set up callback manager to print output word by word\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llama_chat, callback_manager=callback_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2. General question-prompt about the EEG without input context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "question = \"What is electroencephalography?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3. Prompt asking to summarize the input text with __LangChain Stuff__ method\n",
    "- You can specify your own prompt_template_ (A,B,C, ...) but be sure to also change the code below so that the correct template is executed:\n",
    "- ```prompt = PromptTemplate(template=prompt_templateB, input_variables=[\"text\"])```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_documents': [Document(page_content='We will further develop employees in applying the human scale in our\\nservices. With support of artificial intelligence we will make our letters more\\nreadable and make it more understandable. The approach is from the\\noutside in: bottlenecks that our clients experience are mapped on the basis\\nof various forms of (customer) research and analyses. The program\\nInnovation supports initiatives and solution teams effectively organizing\\nimprovement processes and devising innovative ones solutions, with\\nknowledge of the latest (technological) developments and trends. For\\nexample, it is investigated how artificial intelligence (such as ChatGPT) can\\nbe used, for example when rewriting general texts in dozens of letters to\\nimprove readability. Attention to innovation and design thinking also\\ncontributes to the desired results development of a learning organization.\\n', metadata={'source': 'KI_en.pdf', 'page': 0})], 'output_text': '\\n              The company plans to enhance employee development in implementing the \"human scale\" approach in their services. They will utilize artificial intelligence to improve letter readability and comprehension. Research and analysis will identify client bottlenecks, and innovation programs will support solution teams in developing new solutions. Additionally, the company will explore the use of AI in improving text readability, and design thinking will contribute to the desired results.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "prompt_template_A = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "  \"\"\"\n",
    "\n",
    "prompt_template_B = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
    "              ```{text}```\n",
    "              HELPFULL SUMMARY:\n",
    "  \"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template_B, input_variables=[\"text\"])\n",
    "\n",
    "#1) stuff_chain only works when entire document fits into the n_ctx (number of context tokens)\n",
    "stuff_chain = load_summarize_chain(llm= llm, chain_type=\"stuff\",prompt=prompt)\n",
    "\n",
    "loader = PyPDFLoader(\"KI_en.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "try:\n",
    "    output = stuff_chain.invoke(pages)\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(\n",
    "        \"The code failed since it won't be able to run inference on such a huge context and throws this exception: \",\n",
    "        e,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4. Prompt asking to summarize the input text with __LangChain Map-Reduce__ method\n",
    "- Result: As can be seen in the table below the concise_summary for the first document (Greek History) gives gibberish output, speaking about what a summary is but actually not giving the summary about the first input chunk. The second (Belgium History) and third (Dutch History) chunk, do give descent results but i have the feeling the last summary is truncated.\n",
    "\n",
    " \n",
    "  |index | file_name             | file_type\t| page_number\t|         chunks                                    | concise_summary                                 |\n",
    "  |------|-----------------------|--------------|---------------|---------------------------------------------------|----------------------------------------------------------|\n",
    "  | 0\t | history_gr_be_nl\t     | .pdf         |     0\t        | Greek history spans thousands of years and is ... | Summaries are written in complete sentences t...|\n",
    "  | 1    | history_gr_be_nl\t     | .pdf\t        |     1\t        | Belgian history is rich and complex, character... | Summaries of Belgian History.\\n\\nHere are som...|\n",
    "  | 2\t | history_gr_be_nl\t     | .pdf         |     2         | The history of the Netherlands is rich and div... | Here are three summaries based on the provid...          |\n",
    "\n",
    "- I also tried to add the Greek history another time to the document, (so full pdf has 4 pages, Gr, Nl, Be and Gr history snippet) in the hope to see a summary of the last Gr chunk. But i got the same gibberish output as on the pdf with 3 pages.\n",
    "- Reading in the be_gr_nl history pdf instead of the gr_be_nl one gives now for the second chunk (Gr History) the gibberish answer.\n",
    "- CONCLUSION1: This means that there is something wrong with the Greek history input that leads to a gibberish answer.\n",
    "- CONCLUSION2: ```UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset``` Some research is needed in order to benefit from the parallel offload of the map process. All the subdocuments could in principle be processed in parallel and the reduce step could collect the final output from the reduce step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "#from datasets import Dataset\n",
    "\n",
    "loader = PyPDFLoader(\"history2_be_gr_nl.pdf\")\n",
    "#loader = PyPDFLoader(\"history1_be_gr_nl.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# To create a list of strings from each doc in docs you can apply the following peice of code\n",
    "#split_text = [doc.page_content for doc in docs]\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please make summaries. Do not summarize the document when there are no full sentences in the document. \n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Reduce\n",
    "#\"\"\"The following is set of summaries:\n",
    "#{docs}\n",
    "#Take the above summaries and combine them into a final summary \n",
    "#Helpful Answer: \n",
    "#\"\"\"\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_maps}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_maps\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4096\n",
    ")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Combining summaries by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=True\n",
    ")\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=800, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# For .txt files you can use the following text splitter\n",
    "#split_text = text_splitter.split_text(text_input)\n",
    "#print(f\"Number of splits= {len(split_text)}\")\n",
    "\n",
    "# For pdf documents you can use the following documents splitter\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"Number of splits= {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1123: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Elapsed time:  423.55871987342834\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "map_reduce_output = map_reduce_chain.invoke(split_docs)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"\\n\\nElapsed time:  {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_mp_summary = create_pd_from_map_reduce_output(map_reduce_output)\n",
    "\n",
    "# code that prints the output in a human readable format \n",
    "for i in range(len(pdf_mp_summary)):\n",
    "   print(f\"\\n ========================================\\n{pdf_mp_summary.iloc[i]['chunks']} \\n+++++++++++++++++++++++\\n{pdf_mp_summary.iloc[i]['concise_summary']}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_mp_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_reduce_output['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Prompt asking to summarize the input text with __LangChain Refine__ method\n",
    "- (https://python.langchain.com/docs/use_cases/summarization)\n",
    "The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be tested/adjusted to my needs:\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"history_be_gr_nl.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "question_prompt_template = \"\"\"\n",
    "                  Please provide a summary of the following text.\n",
    "                  TEXT: {text}\n",
    "                  SUMMARY:\n",
    "                  \"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "refine_prompt_template = \"\"\"\n",
    "              Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "              \"\"\"\n",
    "\n",
    "refine_prompt = PromptTemplate(\n",
    "    template=refine_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "refine_chain = load_summarize_chain(\n",
    "    llm= llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=question_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=800, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# For .txt files you can use the following text splitter\n",
    "#split_text = text_splitter.split_text(text_input)\n",
    "#print(f\"Number of splits= {len(split_text)}\")\n",
    "\n",
    "# For pdf documents you can use the following documents splitter\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "print(f\"Number of splits= {len(split_docs)}\")\n",
    "\n",
    "refine_outputs = refine_chain({\"input_documents\": split_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be tested/adjusted to my needs:\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "{text}\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "refine_template = (\n",
    "    \"Your job is to produce a text\\n\"\n",
    "    \"We have extracted the first summary from a documnet: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary.\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate.from_template(refine_template)\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    "    input_key=\"input_documents\",\n",
    "    output_key=\"output_text\",\n",
    ")\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=200, chunk_overlap=0\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "result = chain({\"input_documents\": split_docs}, return_only_outputs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 9. LlamaCpp:\n",
    "- Checking if ggfu model has issues to summarize well using the Stuff approach of LangChain\n",
    "  Parameter explanation:\n",
    "  - __temperature=0.1__,   _(default = 0.8)_\n",
    "    <br>Higher temperatures lead to more diverse and varied outputs, as tokens with lower probabilities are more likely to be sampled. Conversely, lower temperatures produce more conservative and deterministic outputs, favoring tokens with higher probabilities.\n",
    "  - __max_tokens=100__,    _(default = 256)_ \n",
    "  <br>The maximum number of tokens considered to generate an answer.After generating the specified number of tokens, the model stops generating additional tokens, ensuring that the output remains within the desired length. It helps prevent the model from generating overly verbose or irrelevant responses.\n",
    "  - __n_ctx=4096__,         _(default = 512)_\n",
    "    <br>It determines the size of the context window or the maximum number of tokens that the model considers when generating or predicting the next token in a sequence. It controls how much past information the model can consider when making predictions.\n",
    "  - __repeat_penalty=1.5__,  _(default =1.1)_\n",
    "    <br>Values between 1.1 and 1.5 are often used to apply a moderate penalty to repeated tokens. Values between 1.5 and 2.0 impose a stronger penalty on repeated tokens, leading to even greater diversity in the generated text. This can be useful when generating longer texts or when minimizing redundancy is a priority. Values greater than 2.0 apply a very strong penalty to repeated tokens, resulting in highly diverse output with minimal repetition. However, setting the repeat_penalty too high may risk reducing the coherence or naturalness of the generated text.\n",
    "  - __last_n_tokens_size = 20__  _(default = 64)_\n",
    "    <br>A larger value means that the model will consider a longer context when determining if a token should be penalized for repetition or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Llama - Cpp: llm instance created with gguf-type file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./llama-2-13b.Q6_K.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False,  # Verbose is required to pass to the callback manager\n",
    "    temperature=0.1, \n",
    "    max_tokens=50,\n",
    "    n_ctx=4096,\n",
    "    repeat_penalty=1.2,\n",
    "    last_n_tokens_size = 96\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Llama Cpp: Stuff Chain (entire input document fits into the llm, no need to split input text):\n",
    "- When the __max_tokens__ parameter of the llm instance > nr of input tokens of the input text --> Full input-text is given as the output (no summarization is made)\n",
    "- When __max_tokens__ < nr of input tokens --> answer = input text truncated at the number of tokens specified by max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. We will further develop employees in applying the human scale in our services.\n",
      "  2. With support of artificial intelligence we will make our letters more readable and make it more understandable.\n",
      "  3. The approach is from the outside in{'input_documents': [Document(page_content='We will further develop employees in applying the human scale in our\\nservices. With support of artificial intelligence we will make our letters more\\nreadable and make it more understandable. The approach is from the\\noutside in: bottlenecks that our clients experience are mapped on the basis\\nof various forms of (customer) research and analyses. The program\\nInnovation supports initiatives and solution teams effectively organizing\\nimprovement processes and devising innovative ones solutions, with\\nknowledge of the latest (technological) developments and trends. For\\nexample, it is investigated how artificial intelligence (such as ChatGPT) can\\nbe used, for example when rewriting general texts in dozens of letters to\\nimprove readability. Attention to innovation and design thinking also\\ncontributes to the desired results development of a learning organization.\\n', metadata={'source': 'KI_en.pdf', 'page': 0})], 'output_text': '1. We will further develop employees in applying the human scale in our services.\\n  2. With support of artificial intelligence we will make our letters more readable and make it more understandable.\\n  3. The approach is from the outside in'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "  \"\"\"\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "#1) stuff_chain only works when entire document fits into the n_ctx (number of context tokens)\n",
    "stuff_chain = load_summarize_chain(llm= llm, chain_type=\"stuff\",prompt=prompt)\n",
    "\n",
    "loader = PyPDFLoader(\"KI_en.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "try:\n",
    "    output = stuff_chain.invoke(pages)\n",
    "    print(output)\n",
    "except Exception as e:\n",
    "    print(\n",
    "        \"The code failed since it won't be able to run inference on such a huge context and throws this exception: \",\n",
    "        e,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Llama Dutch Cpp: llm instance\n",
    "- __Error__: gguf_init_from_file: invalid magic characters ''\n",
    "llama_model_load: error loading model: llama_model_loader: failed to load model from /workspace/Bram/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "n_gpu_layers = -1  # The number of layers to put on the GPU. The rest will be on the CPU. If you don't know how many layers there are, you can use -1 to move all to GPU.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/workspace/Bram/\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=False,  # Verbose is required to pass to the callback manager\n",
    "    temperature=0.1, \n",
    "    max_tokens=200,\n",
    "    n_ctx=4096,\n",
    "    repeat_penalty=1.2,\n",
    "    last_n_tokens_size = 96\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 10. Login to hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add this token in the Token user-input: \"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\"\n",
    "#!pip install huggingface_hub\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\") # christos token\n",
    "\n",
    "# developers@polpo.nl passwd: Polpoai2024@\n",
    "\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add this token in the Token user-input: \"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\"\n",
    "#!pip install huggingface_hub\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 11. Extract nth sentence of the document splits and concatenate them into final output\n",
    "- This was a test to see if Map-Reduce is working. SO the map is extracting the n-th sentence from each subdocument. And the Reduce was to concatenate all those sentences together to one final output. This idea was not fully tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"LandelijkDekkendNetwerk.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Map\n",
    "map_template = \"\"\"Extract the second sentence from each document split:\n",
    "{docs}\n",
    "\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Reduce\n",
    "#\"\"\"The following is set of summaries:\n",
    "#{docs}\n",
    "#Take the above summaries and combine them into a final summary \n",
    "#Helpful Answer: \n",
    "#\"\"\"\n",
    "reduce_template = \"\"\"Concatenate sentences togeter into a single text:\n",
    "{doc_maps}\n",
    "\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_maps\"\n",
    ")\n",
    "\n",
    "# Combines and iteratively reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=512,\n",
    ")\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Combining summaries by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=512, chunk_overlap=0\n",
    ")\n",
    "\n",
    "# For .txt files\n",
    "# text_splitter.split_text(text_input)\n",
    "\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
